{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyPTffTLug7i"
   },
   "source": [
    "# **Laboratorio 11: LLM y Agentes Aut칩nomos 游뱄**\n",
    "\n",
    "MDS7202: Laboratorio de Programaci칩n Cient칤fica para Ciencia de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pbWVyntzbvL"
   },
   "source": [
    "### **Cuerpo Docente:**\n",
    "\n",
    "- Profesores: Ignacio Meza, Sebasti치n Tinoco\n",
    "- Auxiliar: Eduardo Moya\n",
    "- Ayudantes: Nicol치s Ojeda, Melanie Pe침a, Valentina Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy6ikgVYzghB"
   },
   "source": [
    "### **Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser치n revisados**\n",
    "\n",
    "- Nombre de alumno 1: Tom치s Ignacio Reyes Oyarz칰n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMJ-owchzjFf"
   },
   "source": [
    "### **Link de repositorio de GitHub:** [Repo - TR](https://github.com/TomiReyes/MDS7202-TR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUuwsXrKzmkK"
   },
   "source": [
    "## **Temas a tratar**\n",
    "\n",
    "- Reinforcement Learning\n",
    "- Large Language Models\n",
    "\n",
    "## **Reglas:**\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser치n respondidos por este medio.\n",
    "- Prohibidas las copias.\n",
    "- Pueden usar cualquer matrial del curso que estimen conveniente.\n",
    "\n",
    "### **Objetivos principales del laboratorio**\n",
    "\n",
    "- Resoluci칩n de problemas secuenciales usando Reinforcement Learning\n",
    "- Habilitar un Chatbot para entregar respuestas 칰tiles usando Large Language Models.\n",
    "\n",
    "El laboratorio deber치 ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m치ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m치s eficientes que los iteradores nativos sobre DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hmHHQ9BuyAG"
   },
   "source": [
    "## **1. Reinforcement Learning (2.0 puntos)**\n",
    "\n",
    "En esta secci칩n van a usar m칠todos de RL para resolver dos problemas interesantes: `Blackjack` y `LunarLander`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOcejYb6uzOO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: gymnasium[box2d]\n"
     ]
    }
   ],
   "source": [
    "#!pip install -qqq gymnasium stable_baselines3\n",
    "#!pip install -qqq swig\n",
    "#!pip install -qqq gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBPet_Mq8dX9"
   },
   "source": [
    "### **1.1 Blackjack (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.recreoviral.com/wp-content/uploads/2016/08/s3.amazonaws.com-Math.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "La idea de esta subsecci칩n es que puedan implementar m칠todos de RL y as칤 generar una estrategia para jugar el cl치sico juego Blackjack y de paso puedan ~~hacerse millonarios~~ aprender a resolver problemas mediante RL.\n",
    "\n",
    "Comencemos primero preparando el ambiente. El siguiente bloque de c칩digo transforma las observaciones del ambiente a `np.array`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LpZ8bBKk9ZlU"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import MultiDiscrete\n",
    "import numpy as np\n",
    "\n",
    "class FlattenObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(FlattenObservation, self).__init__(env)\n",
    "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.array(observation).flatten()\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "env = FlattenObservation(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ6J1_-Y9nHO"
   },
   "source": [
    "#### **1.1.1 Descripci칩n de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripci칩n sobre el ambiente [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/) y su formulaci칩n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5i1Wt1p770x"
   },
   "source": [
    "El ambiante Blackjack representa el juego donde se compite contra el dealer respecto a quien est치 m치s cerca del n칰mero 21 sumando sus cartas. Dentro del juego se pueden distinguir los siguientes estados: \n",
    "- Suma del n칰mero de las cartas actual del jugador.\n",
    "- Carta del dealer.\n",
    "- Si el jugador tiene un As usable, es decir, que valga 11 si no le hace pasar de 21, de lo contrario vale 1. \n",
    "\n",
    "En las acciones se ve lo siguiente: \n",
    "- Pedir cartas, para acercarse lo m치ximo a 21, si el jugador pasa de ese n칰mero pierde. \n",
    "- Detenerse, no pedir m치s cartas y ver cuanto suma el dealer para comparar. \n",
    "\n",
    "Y en recompesas est치: \n",
    "- Si el jugador gana es +1. \n",
    "- Si el juegador pierde -1.\n",
    "- Si empata con el dealer +/- 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmcX6bRC9agQ"
   },
   "source": [
    "#### **1.1.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci칩n 5000 veces y reporte el promedio y desviaci칩n de las recompensas. 쮺칩mo calificar칤a el performance de esta pol칤tica? 쮺칩mo podr칤a interpretar las recompensas obtenidas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se que no se puede usar ciclor for o while pero al hacer algo 5000 veces la verdad no se me ocurri칩 como hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9p2PrLLR9yju"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensa: -0.4006\n",
      "Desviaci칩n est치ndar de la recompensa: 0.8962810050425033\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 5000\n",
    "rewards = []\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    observation = env.reset()[0]\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  \n",
    "        observation, reward, done, truncated, info = env.step(action)\n",
    "        episode_reward += reward  \n",
    "\n",
    "    rewards.append(episode_reward)  \n",
    "\n",
    "average_reward = np.mean(rewards)\n",
    "std_dev_reward = np.std(rewards)\n",
    "\n",
    "print(f\"Promedio de recompensa: {average_reward}\")\n",
    "print(f\"Desviaci칩n est치ndar de la recompensa: {std_dev_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El promedio de las recompensas siempre se aproxima a -0.4, con una desviaci칩n est치ndar de aprox 0.9. Esto quiere decir que es m치s normal perder que ganar, lo cual hace sentido por como est치 hecho el juego y el prop칩sito que tiene. El hecho de tener una desviaci칩n est치ndar alta (respecto al promedio) indica que s칤 hay casos en que se puede ganar y tener buenos premios, tambi칠n da a entender que hay alta variabilidad por el factor suerte que hay m치s all치 de la estrategia. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEO_dY4x_SJu"
   },
   "source": [
    "#### **1.1.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `Blackjack`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando PPO, por su comportamiento con los problemas discretos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "m9JsFA1wGmnH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.37     |\n",
      "|    ep_rew_mean     | -0.27    |\n",
      "| time/              |          |\n",
      "|    fps             | 4957     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.28        |\n",
      "|    ep_rew_mean          | -0.46       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3362        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018564414 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.676      |\n",
      "|    explained_variance   | -0.0314     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.256       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0546     |\n",
      "|    value_loss           | 0.749       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.39        |\n",
      "|    ep_rew_mean          | -0.16       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3111        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021009888 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.629      |\n",
      "|    explained_variance   | 0.0945      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.224       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0546     |\n",
      "|    value_loss           | 0.719       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.24        |\n",
      "|    ep_rew_mean          | -0.14       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2995        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027563399 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.562      |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.299       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0388     |\n",
      "|    value_loss           | 0.709       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.29       |\n",
      "|    ep_rew_mean          | -0.08      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 2932       |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01667795 |\n",
      "|    clip_fraction        | 0.11       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.496     |\n",
      "|    explained_variance   | 0.175      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.307      |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0201    |\n",
      "|    value_loss           | 0.722      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x156e69d90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-bpdb8wZID1"
   },
   "source": [
    "#### **1.1.4 Evaluaci칩n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.1.2 pero utilizando el modelo entrenado. 쮺칩mo es el performance de su agente? 쮼s mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "5-d7d8GFf7F6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio: -0.1152\n",
      "Desviaci칩n est치ndar de la recompensa: 0.9605878200352116\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=5000)\n",
    "\n",
    "print(f\"Recompensa promedio: {mean_reward}\")\n",
    "print(f\"Desviaci칩n est치ndar de la recompensa: {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa una disminuci칩n considerable en el promedio de la recompensa, pasando de un aproximado -0.4 a un -0.1, donde sigue siendo m치s probable perder en el largo plazo, pero por el alto valor del desviaci칩n est치ndar sigue existiendo alta variabilidad, dandole sentido al factor suerte y las probabilidades de obtener recompensas en el juego. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO-EsAaPAYEm"
   },
   "source": [
    "#### **1.1.5 Estudio de acciones (0.2 puntos)**\n",
    "\n",
    "Genere una funci칩n que reciba un estado y retorne la accion del agente. Luego, use esta funci칩n para entregar la acci칩n escogida frente a los siguientes escenarios:\n",
    "\n",
    "- Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene tiene un as\n",
    "- Suma de cartas del agente es 19, dealer muestra un 3, agente tiene tiene un as\n",
    "\n",
    "쯉on coherentes sus acciones con las reglas del juego?\n",
    "\n",
    "Hint: 쮸 que clase de python pertenecen los estados? Pruebe a usar el m칠todo `.reset` para saberlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fh8XlGyzwtRp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase del estado: <class 'numpy.ndarray'>, Estado de ejemplo: [19 10  0]\n"
     ]
    }
   ],
   "source": [
    "def obtener_accion(model, estado):\n",
    "    accion, _ = model.predict([estado], deterministic=True)\n",
    "    return accion[0]  #\n",
    "\n",
    "estado = env.reset()[0]\n",
    "print(f\"Clase del estado: {type(estado)}, Estado de ejemplo: {estado}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acci칩n para el escenario 1: 1\n",
      "Acci칩n para el escenario 2: 0\n"
     ]
    }
   ],
   "source": [
    "estado1 = (6, 7, False)\n",
    "accion1 = obtener_accion(model, estado1)\n",
    "print(f\"Acci칩n para el escenario 1: {accion1}\")\n",
    "\n",
    "estado2 = (19, 3, True)\n",
    "accion2 = obtener_accion(model, estado2)\n",
    "print(f\"Acci칩n para el escenario 2: {accion2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son coherentes las desiciones que se tomaron dado que pidi칩 carta cuando ten칤a un 6, que es una mano baja y con altas probabilidades de perder, y no pidi칩 carta cuando ten칤a un 19 que es una buena mano y es muy probable que pierde si pide otra carta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEqCTqqroh03"
   },
   "source": [
    "### **1.2 LunarLander**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.redd.it/097t6tk29zf51.jpg\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la secci칩n 2.1, en esta secci칩n usted se encargar치 de implementar una gente de RL que pueda resolver el ambiente `LunarLander`.\n",
    "\n",
    "Comencemos preparando el ambiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "nvQUyuZ_FtZ4"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode = \"rgb_array\", continuous = True) # notar el par치metro continuous = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBU4lGX3wpN6"
   },
   "source": [
    "Noten que se especifica el par치metro `continuous = True`. 쯈ue implicancias tiene esto sobre el ambiente?\n",
    "\n",
    "Adem치s, se le facilita la funci칩n `export_gif` para el ejercicio 2.2.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "bRiWpSo9yfr9"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "def export_gif(model, n = 5):\n",
    "  '''\n",
    "  funci칩n que exporta a gif el comportamiento del agente en n episodios\n",
    "  '''\n",
    "  images = []\n",
    "  for episode in range(n):\n",
    "    obs = model.env.reset()\n",
    "    img = model.env.render()\n",
    "    done = False\n",
    "    while not done:\n",
    "      images.append(img)\n",
    "      action, _ = model.predict(obs)\n",
    "      obs, reward, done, info = model.env.step(action)\n",
    "      img = model.env.render(mode=\"rgb_array\")\n",
    "\n",
    "  imageio.mimsave(\"agent_performance.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk5VJVppXh3N"
   },
   "source": [
    "#### **1.2.1 Descripci칩n de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripci칩n sobre el ambiente [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) y su formulaci칩n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas. 쮺omo se distinguen las acciones de este ambiente en comparaci칩n a `Blackjack`?\n",
    "\n",
    "Nota: recuerde que se especific칩 el par치metro `continuous = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb-u9LUE8O9a"
   },
   "source": [
    "LunarLander es un juego de aterrizar una nave de manera correcta en el espacio que corresponde. Dentro del juego se pueden distinguir los siguientes estados: \n",
    "- Posici칩n de la nave en el eje x y eje y.\n",
    "- Velocidades en x e y.\n",
    "- 츼ngulo de orientaci칩n y velocidad angular. \n",
    "- Indicador si las patas de aterrizaje izquierda y derecha tocaron el suelo. \n",
    "\n",
    "Considerando el modo continuo, las acciones son: \n",
    "- Impulso del motor vertical \n",
    "- Impulso de motores horizontales. \n",
    "\n",
    "Y en recompesas est치: \n",
    "- Recompensa por acercarse a la plataforma y disminuir la velocidad.\n",
    "- +10 puntos por cada pata que toque el suelo.\n",
    "- -0.3 por uso del motor principal, -0.03 por uso de motores horizontales.\n",
    "- +100 puntos aterrizaje exitoso.\n",
    "- -100 puntos por chocar. \n",
    "\n",
    "En este ambiente existen m치s escenarios, estados, acciones y recompensas que en el blackjack, porque el juego de azar es un ambiente discreto mientras que LunarLander trata de un ambiente continuo con m치s desiciones que tomar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YChodtNQwzG2"
   },
   "source": [
    "#### **1.2.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci칩n 10 veces y reporte el promedio y desviaci칩n de las recompensas. 쮺칩mo calificar칤a el performance de esta pol칤tica?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bwc3A0GX7a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensa: -236.68379621334884\n",
      "Desviaci칩n est치ndar de la recompensa: 101.19090565970589\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "rewards = []\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    observation = env.reset()[0]\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "        episode_reward += reward  \n",
    "    rewards.append(episode_reward) \n",
    "\n",
    "average_reward = np.mean(rewards)\n",
    "std_dev_reward = np.std(rewards)\n",
    "\n",
    "print(f\"Promedio de recompensa: {average_reward}\")\n",
    "print(f\"Desviaci칩n est치ndar de la recompensa: {std_dev_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se realiza de manera aleatoria, no tiene sentido dado que se penaliza bastante el chocar o tomar desiciones al azar. Esto se explica con el promedio de recompensa que se aproxima a -250, con una desviaci칩n est치ndar cercana a 100, quiere decir que en la mayor칤a de los casos se pierde, esto porque se requiere tener m치s sentido en las desiciones al tratar de un juego continuo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQrZVQflX_5f"
   },
   "source": [
    "#### **1.2.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `LunarLander` **usando 10000 timesteps de entrenamiento**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_6Ia9uoF7Hs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 106      |\n",
      "|    ep_rew_mean     | -153     |\n",
      "| time/              |          |\n",
      "|    fps             | 3994     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 108         |\n",
      "|    ep_rew_mean          | -166        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3017        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004465096 |\n",
      "|    clip_fraction        | 0.0334      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.00285    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 488         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00771    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.37e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 110          |\n",
      "|    ep_rew_mean          | -166         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2808         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034254668 |\n",
      "|    clip_fraction        | 0.0169       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | -0.0173      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 503          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0049      |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 1.25e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 112          |\n",
      "|    ep_rew_mean          | -168         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2730         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071663945 |\n",
      "|    clip_fraction        | 0.0333       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.86        |\n",
      "|    explained_variance   | -0.0466      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 646          |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00498     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 1.2e+03      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 111          |\n",
      "|    ep_rew_mean          | -174         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2681         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044424823 |\n",
      "|    clip_fraction        | 0.0165       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.86        |\n",
      "|    explained_variance   | -0.00352     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 278          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00584     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 812          |\n",
      "------------------------------------------\n",
      "Recompensa promedio: -172.95964538636616\n",
      "Desviaci칩n est치ndar de la recompensa: 121.28234769768235\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3z-oIUSrlAsY"
   },
   "source": [
    "#### **1.2.4 Evaluaci칩n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.2.2 pero utilizando el modelo entrenado. 쮺칩mo es el performance de su agente? 쮼s mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ophyU3KrWrwl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio: -155.72273620029446\n",
      "Desviaci칩n est치ndar de la recompensa: 96.92851277984472\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Recompensa promedio: {mean_reward}\")\n",
    "print(f\"Desviaci칩n est치ndar de la recompensa: {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es mejor que el escenario baseline pero sigue siendo negativo, al tratar de un ejercicio continuo puede que el modelo necesite m치s tiempo para aprender de mejor manera. De todos modos, aument칩 la recompensa de -250 a -150 aprox. Con la misma desviaci칩n est치ndar, por lo que el juego a칰n tiene variabilidad que afecta en su desempe침o."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6Xw4YHT3P5d"
   },
   "source": [
    "#### **1.2.5 Optimizaci칩n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita los ejercicios 1.2.3 y 1.2.4 hasta obtener un nivel de recompensas promedio mayor a 50. Para esto, puede cambiar manualmente par치metros como:\n",
    "- `total_timesteps`\n",
    "- `learning_rate`\n",
    "- `batch_size`\n",
    "\n",
    "Una vez optimizado el modelo, use la funci칩n `export_gif` para estudiar el comportamiento de su agente en la resoluci칩n del ambiente y comente sobre sus resultados.\n",
    "\n",
    "Adjunte el gif generado en su entrega (mejor a칰n si adem치s adjuntan el gif en el markdown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aItYF6sr6F_6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 117      |\n",
      "|    ep_rew_mean     | -281     |\n",
      "| time/              |          |\n",
      "|    fps             | 3953     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 111         |\n",
      "|    ep_rew_mean          | -263        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2581        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007455999 |\n",
      "|    clip_fraction        | 0.0477      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.86       |\n",
      "|    explained_variance   | -0.00574    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.22e+03    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00435    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 1.87e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 109         |\n",
      "|    ep_rew_mean          | -264        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2274        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014346846 |\n",
      "|    clip_fraction        | 0.0628      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | -0.00197    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 447         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0036     |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 1.32e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 112         |\n",
      "|    ep_rew_mean          | -253        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2159        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014127588 |\n",
      "|    clip_fraction        | 0.0587      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.88       |\n",
      "|    explained_variance   | -0.00635    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 380         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00285    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 1.51e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 117         |\n",
      "|    ep_rew_mean          | -266        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2069        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008677239 |\n",
      "|    clip_fraction        | 0.0865      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.89       |\n",
      "|    explained_variance   | -0.00143    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 270         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.000326    |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 693         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 119          |\n",
      "|    ep_rew_mean          | -247         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2011         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071522696 |\n",
      "|    clip_fraction        | 0.0499       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.91        |\n",
      "|    explained_variance   | -0.000664    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 222          |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 1.01e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 122          |\n",
      "|    ep_rew_mean          | -240         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1962         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060688914 |\n",
      "|    clip_fraction        | 0.0879       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.95        |\n",
      "|    explained_variance   | -2.71e-05    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 107          |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00331     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 296          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 123          |\n",
      "|    ep_rew_mean          | -220         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1939         |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059807533 |\n",
      "|    clip_fraction        | 0.0638       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.93        |\n",
      "|    explained_variance   | -1.73e-05    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 332          |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0032      |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 753          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 126         |\n",
      "|    ep_rew_mean          | -201        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1924        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008148523 |\n",
      "|    clip_fraction        | 0.0863      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.93       |\n",
      "|    explained_variance   | -7.15e-07   |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 198         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00647    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 427         |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 126       |\n",
      "|    ep_rew_mean          | -172      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 1906      |\n",
      "|    iterations           | 10        |\n",
      "|    time_elapsed         | 10        |\n",
      "|    total_timesteps      | 20480     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0057433 |\n",
      "|    clip_fraction        | 0.0813    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.92     |\n",
      "|    explained_variance   | -1.07e-06 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 132       |\n",
      "|    n_updates            | 90        |\n",
      "|    policy_gradient_loss | -0.00671  |\n",
      "|    std                  | 1.04      |\n",
      "|    value_loss           | 295       |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 125         |\n",
      "|    ep_rew_mean          | -142        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1882        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015755914 |\n",
      "|    clip_fraction        | 0.0808      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.92       |\n",
      "|    explained_variance   | -5.13e-06   |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 211         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00188    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 327         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 126         |\n",
      "|    ep_rew_mean          | -134        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1872        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004406758 |\n",
      "|    clip_fraction        | 0.0731      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.89       |\n",
      "|    explained_variance   | 2.32e-06    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 211         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00399    |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 422         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 134         |\n",
      "|    ep_rew_mean          | -134        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1664        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005114861 |\n",
      "|    clip_fraction        | 0.0242      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.89       |\n",
      "|    explained_variance   | 0.0324      |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 229         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00412    |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 393         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 138         |\n",
      "|    ep_rew_mean          | -120        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1668        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005577782 |\n",
      "|    clip_fraction        | 0.0555      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.89       |\n",
      "|    explained_variance   | 0.172       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 147         |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00926    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 301         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 150          |\n",
      "|    ep_rew_mean          | -115         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1573         |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068000676 |\n",
      "|    clip_fraction        | 0.0422       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.89        |\n",
      "|    explained_variance   | 0.278        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 58.3         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00661     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 249          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 152          |\n",
      "|    ep_rew_mean          | -104         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1583         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047580325 |\n",
      "|    clip_fraction        | 0.0391       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.89        |\n",
      "|    explained_variance   | 0.361        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 39.9         |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00382     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 291          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 157         |\n",
      "|    ep_rew_mean          | -99.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1583        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005592329 |\n",
      "|    clip_fraction        | 0.0458      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.9        |\n",
      "|    explained_variance   | 0.482       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 85.9        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00662    |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 139         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 164          |\n",
      "|    ep_rew_mean          | -96.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1546         |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076021384 |\n",
      "|    clip_fraction        | 0.0695       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.9         |\n",
      "|    explained_variance   | 0.326        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 152          |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0119      |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 350          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 177          |\n",
      "|    ep_rew_mean          | -87.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1500         |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055795424 |\n",
      "|    clip_fraction        | 0.0513       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.91        |\n",
      "|    explained_variance   | 0.736        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 52.4         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00523     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 104          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 182         |\n",
      "|    ep_rew_mean          | -81         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1486        |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008908619 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.9        |\n",
      "|    explained_variance   | 0.748       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 52.4        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 98          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 197         |\n",
      "|    ep_rew_mean          | -78.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1431        |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009169552 |\n",
      "|    clip_fraction        | 0.0806      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.88       |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 22.1        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0068     |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 78.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 217         |\n",
      "|    ep_rew_mean          | -73.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1358        |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007789369 |\n",
      "|    clip_fraction        | 0.0814      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 16.9        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00416    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 84          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 230         |\n",
      "|    ep_rew_mean          | -66.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1317        |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015333308 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.8        |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 7.54        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    std                  | 0.97        |\n",
      "|    value_loss           | 39.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 234         |\n",
      "|    ep_rew_mean          | -56.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1296        |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013030836 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.74       |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 21.9        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    std                  | 0.938       |\n",
      "|    value_loss           | 54.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 247          |\n",
      "|    ep_rew_mean          | -53.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1255         |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 40           |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073054237 |\n",
      "|    clip_fraction        | 0.0895       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 18.6         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00424     |\n",
      "|    std                  | 0.934        |\n",
      "|    value_loss           | 40.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 263         |\n",
      "|    ep_rew_mean          | -49         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1225        |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011356565 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.69       |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 14          |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00659    |\n",
      "|    std                  | 0.931       |\n",
      "|    value_loss           | 58.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 277         |\n",
      "|    ep_rew_mean          | -47.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1193        |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011148443 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.65       |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 12.5        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00892    |\n",
      "|    std                  | 0.901       |\n",
      "|    value_loss           | 37.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 291         |\n",
      "|    ep_rew_mean          | -39.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1152        |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011850733 |\n",
      "|    clip_fraction        | 0.0895      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.6        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.76        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00175    |\n",
      "|    std                  | 0.88        |\n",
      "|    value_loss           | 34.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 308         |\n",
      "|    ep_rew_mean          | -37.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1126        |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008032192 |\n",
      "|    clip_fraction        | 0.0914      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.59       |\n",
      "|    explained_variance   | 0.801       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 6.54        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00388    |\n",
      "|    std                  | 0.886       |\n",
      "|    value_loss           | 107         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 321         |\n",
      "|    ep_rew_mean          | -32.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1105        |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009597252 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.57       |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 12.7        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00391    |\n",
      "|    std                  | 0.873       |\n",
      "|    value_loss           | 103         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 327         |\n",
      "|    ep_rew_mean          | -31.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1100        |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007125445 |\n",
      "|    clip_fraction        | 0.0616      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.53       |\n",
      "|    explained_variance   | 0.773       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 71.8        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00516    |\n",
      "|    std                  | 0.847       |\n",
      "|    value_loss           | 165         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 343         |\n",
      "|    ep_rew_mean          | -30.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1090        |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006693977 |\n",
      "|    clip_fraction        | 0.0882      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.51       |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 66          |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00613    |\n",
      "|    std                  | 0.855       |\n",
      "|    value_loss           | 60.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 352         |\n",
      "|    ep_rew_mean          | -29.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1084        |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008458933 |\n",
      "|    clip_fraction        | 0.0819      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.51       |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 169         |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00319    |\n",
      "|    std                  | 0.845       |\n",
      "|    value_loss           | 113         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 372        |\n",
      "|    ep_rew_mean          | -25.5      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1077       |\n",
      "|    iterations           | 34         |\n",
      "|    time_elapsed         | 64         |\n",
      "|    total_timesteps      | 69632      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00971023 |\n",
      "|    clip_fraction        | 0.0915     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.53      |\n",
      "|    explained_variance   | 0.931      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 55.8       |\n",
      "|    n_updates            | 330        |\n",
      "|    policy_gradient_loss | -0.0066    |\n",
      "|    std                  | 0.868      |\n",
      "|    value_loss           | 61.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 390         |\n",
      "|    ep_rew_mean          | -24         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1056        |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 67          |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005721221 |\n",
      "|    clip_fraction        | 0.0846      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.53       |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 7.56        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.000487   |\n",
      "|    std                  | 0.858       |\n",
      "|    value_loss           | 112         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 403         |\n",
      "|    ep_rew_mean          | -18.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1054        |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 69          |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008174409 |\n",
      "|    clip_fraction        | 0.0898      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.49       |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.6         |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00233    |\n",
      "|    std                  | 0.836       |\n",
      "|    value_loss           | 15.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 414          |\n",
      "|    ep_rew_mean          | -14.8        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1039         |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 72           |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056904564 |\n",
      "|    clip_fraction        | 0.0561       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.844        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 191          |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00391     |\n",
      "|    std                  | 0.833        |\n",
      "|    value_loss           | 119          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 432         |\n",
      "|    ep_rew_mean          | -7.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1037        |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 74          |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008249228 |\n",
      "|    clip_fraction        | 0.0795      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.46       |\n",
      "|    explained_variance   | 0.649       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 114         |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00715    |\n",
      "|    std                  | 0.833       |\n",
      "|    value_loss           | 189         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 448          |\n",
      "|    ep_rew_mean          | -1.81        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1030         |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 77           |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067386613 |\n",
      "|    clip_fraction        | 0.0468       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.45        |\n",
      "|    explained_variance   | 0.736        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 171          |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.00258     |\n",
      "|    std                  | 0.824        |\n",
      "|    value_loss           | 320          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 456         |\n",
      "|    ep_rew_mean          | 3.28        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1025        |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006938911 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.42       |\n",
      "|    explained_variance   | 0.77        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 20.9        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00165    |\n",
      "|    std                  | 0.816       |\n",
      "|    value_loss           | 104         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 460          |\n",
      "|    ep_rew_mean          | 15.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1027         |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 81           |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076231314 |\n",
      "|    clip_fraction        | 0.0949       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.41        |\n",
      "|    explained_variance   | 0.849        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 21.4         |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00464     |\n",
      "|    std                  | 0.808        |\n",
      "|    value_loss           | 95.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 476          |\n",
      "|    ep_rew_mean          | 18.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1018         |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 84           |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0108247455 |\n",
      "|    clip_fraction        | 0.0765       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.38        |\n",
      "|    explained_variance   | 0.666        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 25           |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00743     |\n",
      "|    std                  | 0.79         |\n",
      "|    value_loss           | 190          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 487         |\n",
      "|    ep_rew_mean          | 24.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1011        |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 87          |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011545405 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.34       |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 21.6        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00396    |\n",
      "|    std                  | 0.784       |\n",
      "|    value_loss           | 131         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 503          |\n",
      "|    ep_rew_mean          | 33.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1009         |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 89           |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071927104 |\n",
      "|    clip_fraction        | 0.0811       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.35        |\n",
      "|    explained_variance   | 0.835        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 13.8         |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | 0.000288     |\n",
      "|    std                  | 0.785        |\n",
      "|    value_loss           | 124          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 506          |\n",
      "|    ep_rew_mean          | 40.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1004         |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 91           |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062534343 |\n",
      "|    clip_fraction        | 0.0795       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.33        |\n",
      "|    explained_variance   | 0.828        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 26.3         |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00245     |\n",
      "|    std                  | 0.777        |\n",
      "|    value_loss           | 106          |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 514        |\n",
      "|    ep_rew_mean          | 40.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1000       |\n",
      "|    iterations           | 46         |\n",
      "|    time_elapsed         | 94         |\n",
      "|    total_timesteps      | 94208      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01172702 |\n",
      "|    clip_fraction        | 0.114      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.27      |\n",
      "|    explained_variance   | 0.792      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 29.5       |\n",
      "|    n_updates            | 450        |\n",
      "|    policy_gradient_loss | -0.00501   |\n",
      "|    std                  | 0.747      |\n",
      "|    value_loss           | 120        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 511         |\n",
      "|    ep_rew_mean          | 38          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 998         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 96          |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009133062 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.25       |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 7.14        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.00678    |\n",
      "|    std                  | 0.754       |\n",
      "|    value_loss           | 110         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 515         |\n",
      "|    ep_rew_mean          | 45.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 995         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 98          |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007131334 |\n",
      "|    clip_fraction        | 0.0674      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.26       |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 42.1        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0033     |\n",
      "|    std                  | 0.756       |\n",
      "|    value_loss           | 191         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 528         |\n",
      "|    ep_rew_mean          | 53.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 989         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 101         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010556026 |\n",
      "|    clip_fraction        | 0.0981      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.24       |\n",
      "|    explained_variance   | 0.773       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 65.6        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.000774   |\n",
      "|    std                  | 0.741       |\n",
      "|    value_loss           | 157         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio: 120.71579263039924\n",
      "Desviaci칩n est치ndar de la recompensa: 82.10914774619216\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-4  \n",
    "batch_size = 32       \n",
    "total_timesteps = 100000 \n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, learning_rate=learning_rate, batch_size=batch_size)\n",
    "\n",
    "model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Recompensa promedio: {mean_reward}\")\n",
    "print(f\"Desviaci칩n est치ndar de la recompensa: {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_gif(model, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Agent Performance GIF](agent_performance.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPUY-Ktgf2BO"
   },
   "source": [
    "## **2. Large Language Models (4.0 puntos)**\n",
    "\n",
    "En esta secci칩n se enfocar치n en habilitar un Chatbot que nos permita responder preguntas 칰tiles a trav칠s de LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ4fPRRihGLe"
   },
   "source": [
    "### **2.0 Configuraci칩n Inicial**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/uqAs9atZH58AAAAd/config-config-issue.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Como siempre, cargamos todas nuestras API KEY al entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ud2Xm_k-hFJn"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj9JvQUsgZZJ"
   },
   "source": [
    "### **2.1 Retrieval Augmented Generation (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://y.yarn.co/218aaa02-c47e-4ec9-b1c9-07792a06a88f_text.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsecci칩n es que habiliten un chatbot que pueda responder preguntas usando informaci칩n contenida en documentos PDF a trav칠s de **Retrieval Augmented Generation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrxOQroVnaZ5"
   },
   "source": [
    "#### **2.1.1 Reunir Documentos (0 puntos)**\n",
    "\n",
    "Reuna documentos PDF sobre los que hacer preguntas siguiendo las siguientes instrucciones:\n",
    "  - 2 documentos .pdf como m칤nimo.\n",
    "  - 50 p치ginas de contenido como m칤nimo entre todos los documentos.\n",
    "  - Ideas para documentos: Documentos relacionados a temas acad칠micos, laborales o de ocio. Aprovechen este ejercicio para construir algo 칰til y/o relevante para ustedes!\n",
    "  - Deben ocupar documentos reales, no pueden utilizar los mismos de la clase.\n",
    "  - Deben registrar sus documentos en la siguiente [planilla](https://docs.google.com/spreadsheets/d/1Hy1w_dOiG2UCHJ8muyxhdKPZEPrrL7BNHm6E90imIIM/edit?usp=sharing). **NO PUEDEN USAR LOS MISMOS DOCUMENTOS QUE OTRO GRUPO**\n",
    "  - **Recuerden adjuntar los documentos en su entrega**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5D1tIRCi4oJJ",
    "outputId": "39f6d4fc-63cb-4b9b-d48f-48d60df25ee5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install --upgrade --quiet PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kzq2TjWCnu15"
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "doc_paths = ['/Users/tomasreyes/Desktop/Cosas U /Laboratorio de Programaci칩n Cient칤fica/MDS7202-1/pdfs/analysing-soccer-cluster-concepts.pdf', \n",
    "             \"/Users/tomasreyes/Desktop/Cosas U /Laboratorio de Programaci칩n Cient칤fica/MDS7202-1/pdfs/Clustering_soccer_players_investigating.pdf\", \n",
    "             \"/Users/tomasreyes/Desktop/Cosas U /Laboratorio de Programaci칩n Cient칤fica/MDS7202-1/pdfs/ml-approach-xg.pdf\"] # rellenar con los path a sus documentos\n",
    "\n",
    "assert len(doc_paths) >= 2, \"Deben adjuntar un m칤nimo de 2 documentos\"\n",
    "\n",
    "total_paginas = sum(len(PyPDF2.PdfReader(open(doc, \"rb\")).pages) for doc in doc_paths)\n",
    "assert total_paginas >= 50, f\"P치ginas insuficientes: {total_paginas}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r811-P71nizA"
   },
   "source": [
    "#### **2.1.2 Vectorizar Documentos (0.2 puntos)**\n",
    "\n",
    "Vectorice los documentos y almacene sus representaciones de manera acorde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "n-yXAdCSn4JM"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader1 = PyPDFLoader(doc_paths[0])\n",
    "loader2 = PyPDFLoader(doc_paths[1])\n",
    "loader3 = PyPDFLoader(doc_paths[2])\n",
    "\n",
    "docs = loader1.load() + loader2.load() + loader3.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50) \n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x12e67a410>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\") \n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embedding) \n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAUkP5zrnyBK"
   },
   "source": [
    "#### **2.1.3 Habilitar RAG (0.3 puntos)**\n",
    "\n",
    "Habilite la soluci칩n RAG a trav칠s de una *chain* y gu치rdela en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", \n",
    "                                     search_kwargs={\"k\": 3}, # n춿 documentos a recuperar\n",
    "                                     )\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"Usando el contexto siguiente:\\n{context}\\nResponde la pregunta: {question}\"\n",
    ")\n",
    "\n",
    "qa_chain = load_qa_chain(llm=llm, chain_type=\"stuff\", prompt=prompt_template)\n",
    "\n",
    "rag_chain = RetrievalQA(\n",
    "    retriever=retriever,\n",
    "    combine_documents_chain=qa_chain\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycg5S5i_n-kL"
   },
   "source": [
    "#### **2.1.4 Verificaci칩n de respuestas (0.5 puntos)**\n",
    "\n",
    "Genere un listado de 3 tuplas (\"pregunta\", \"respuesta correcta\") y analice la respuesta de su soluci칩n para cada una. 쯉u soluci칩n RAG entrega las respuestas que esperaba?\n",
    "\n",
    "Ejemplo de tupla:\n",
    "- Pregunta: 쯈ui칠n es el presidente de Chile?\n",
    "- Respuesta correcta: El presidente de Chile es Gabriel Boric\n",
    "\n",
    "Caso1: \n",
    "- Pregunta: 쮺u치l es el objetivo principal del documento sobre Expected Goals (xG)?\n",
    "- Respuesta correcta: Desarrollar un modelo de Expected Goals (xG) ajustado para cada posici칩n y jugador en f칰tbol, mejorando la precisi칩n en la predicci칩n de goles seg칰n datos de StatsBomb.\n",
    "\n",
    "Caso2: \n",
    "- Pregunta: 쯈u칠 m칠todos de clustering se utilizan en el documento de clustering de jugadores de f칰tbol?\n",
    "- Respuesta correcta: El documento utiliza K-means y Expectation Maximization (EM) para agrupar a jugadores seg칰n su posici칩n, evaluando la pureza y el puntaje de silhouette.\n",
    "\n",
    "Caso3: \n",
    "- Pregunta: 쯈u칠 es un \"conceptor\" en el contexto del an치lisis de juegos de f칰tbol?\n",
    "- Respuesta correcta: Un \"conceptor\" es una herramienta de redes neuronales recurrentes que ayuda a identificar y predecir patrones repetitivos en los juegos de f칰tbol simulados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "S_UiEn1hoZYR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '쮺u치l es el objetivo principal del documento sobre Expected Goals (xG)?', 'result': 'El objetivo principal del documento es crear y aplicar un modelo de Expected Goals (xG) \"desde cero\" para evaluar el rendimiento de equipos e individuos.\\n'}\n"
     ]
    }
   ],
   "source": [
    "respuesta1 = rag_chain.invoke({\"query\": \"쮺u치l es el objetivo principal del documento sobre Expected Goals (xG)?\"})\n",
    "print(respuesta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '쯈u칠 m칠todos de clustering se utilizan en el documento de clustering de jugadores de f칰tbol?', 'result': 'El documento utiliza dos m칠todos de clustering: K-means y Expectation Maximization (EM).\\n'}\n"
     ]
    }
   ],
   "source": [
    "respuesta2 = rag_chain.invoke({\"query\": \"쯈u칠 m칠todos de clustering se utilizan en el documento de clustering de jugadores de f칰tbol?\"})\n",
    "print(respuesta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En el contexto del an치lisis de juegos de f칰tbol, un \"conceptor\" es una matriz de baja dimensionalidad (una variedad de baja dimensi칩n) que describe trayectorias a trav칠s de un espacio de estados de alta dimensionalidad.  Estos permiten predicciones espec칤ficas de la situaci칩n a partir de la misma red neuronal recurrente.  En esencia, los conceptores restringen la din치mica de la red neuronal a un subespacio lineal caracter칤stico de un patr칩n espec칤fico (como un movimiento en el juego), permitiendo identificar y analizar movimientos como \"pase\" o \"regate\" sin necesidad de definirlos previamente.  Permiten segmentar el juego en secuencias de situaciones aprendidas de forma no supervisada y predecir el futuro cercano de cada situaci칩n.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "respuesta3 = rag_chain.run({\"query\": \"쯈u칠 es un 'conceptor' en el contexto del an치lisis de juegos de f칰tbol?\"})\n",
    "print(respuesta3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8d5zTMHoUgF"
   },
   "source": [
    "#### **2.1.5 Sensibilidad de Hiperpar치metros (0.5 puntos)**\n",
    "\n",
    "Extienda el an치lisis del punto 2.1.4 analizando c칩mo cambian las respuestas entregadas cambiando los siguientes hiperpar치metros:\n",
    "- `Tama침o del chunk`. (*쮺칩mo repercute que los chunks sean mas grandes o chicos?*)\n",
    "- `La cantidad de chunks recuperados`. (*쯈u칠 pasa si se devuelven muchos/pocos chunks?*)\n",
    "- `El tipo de b칰squeda`. (*쮺칩mo afecta el tipo de b칰squeda a las respuestas de mi RAG?*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tama침o del chunk: El tama침o del chunk afecta en la vectorizaci칩n de los documentos por lo que si el tama침o del chunk es menor, afecta en la eficiencia del modelo. Ahora pasando a las respuestas: con chunks m치s grandes (1000 en este caso) la respuesta es m치s larga aunque las dos tienen informaci칩n similar, la respuesta con chunk de 500 es m치s precisa, por lo tanto, es mejor. Comparando con chunks de menor tama침o (100 en este caso) la respuesta es m치s corta y pierde harta informaci칩n, por lo que no es correcta o est치 muy incompleta.\n",
    "\n",
    "    - Chunk de 1000: 'El objetivo principal del documento es crear y aplicar un modelo de Expected Goals (xG) \"desde cero\" para predecir xG,  desarrollar m칠tricas xG ajustadas por posici칩n y jugador, y proporcionar ventajas competitivas a la industria y mejoras en el conocimiento acad칠mico.  Esto incluye la creaci칩n de un modelo confiable y robusto que pueda predecir goles con precisi칩n, evaluar la calidad del remate de los jugadores y lograr una correlaci칩n positiva significativa con los proveedores de xG de la industria.\\n'\n",
    "\n",
    "    - Chunk de 500 (utilizado): 'El objetivo principal del documento es crear y aplicar un modelo de Expected Goals (xG) \"desde cero\", utilizando caracter칤sticas nuevas y altamente informativas para predecir los valores de xG.  Tambi칠n busca proporcionar ventajas competitivas a la industria y mejorar el conocimiento acad칠mico a trav칠s de la publicaci칩n de un nuevo modelo de xG ajustado por posici칩n y jugador.\\n'\n",
    "\n",
    "    - Chunk de 100: El objetivo principal del documento es crear y aplicar un modelo de Expected Goals (xG) \"desde cero\" para evaluar el rendimiento de equipos e individuos\n",
    "\n",
    "\n",
    "* Cantidad de chunks recuperados: La cantidad de chunks recuperados afecta en la complejidad de la respuesta dado que define cuantos fragmentos se van a utilizar para realizar la respuesta. Con m치s chunks la respuesta deber칤a ser m치s completa pero con riesgo de tener informaci칩n irrelevante, con menos chunks puede que no se logre obtener la informaci칩n esperada\n",
    "\n",
    "    - 1 recuperado: 'El texto no especifica los m칠todos de clustering utilizados.  Solo menciona que se agrupan los jugadores en 4 y 11 clusters bas치ndose en su posici칩n (portero, defensa, mediocampista, atacante) y la frecuencia de eventos que realizan.  No se nombra ning칰n algoritmo de clustering espec칤fico (como k-means, jer치rquico, etc.).\\n'\n",
    "\n",
    "    - 3 recuperados: 'El documento utiliza dos m칠todos de clustering: K-means y Expectation Maximization (EM)'\n",
    "\n",
    "    - 5 recuperados: 'El documento utiliza dos m칠todos de clustering no supervisados: K-means y Expectation Maximization (EM).\\n'\n",
    "\n",
    "\n",
    "* Tipo de b칰squeda: El tipo de b칰squeda define como se van a seleccionar los chunks para buscar la respuesta, aqu칤 las respuestas con 3 distintos tipos de b칰squeda:\n",
    "\n",
    "    - Similarity: 'En el contexto del an치lisis de juegos de f칰tbol, un \"conceptor\" es una matriz de baja dimensionalidad (una variedad de baja dimensi칩n) que describe trayectorias a trav칠s de un espacio de estados de alta dimensionalidad.  Estos permiten predicciones espec칤ficas de la situaci칩n a partir de la misma red neuronal recurrente.  En esencia, los conceptores restringen la din치mica de la red neuronal a un subespacio lineal caracter칤stico de un patr칩n espec칤fico (como un movimiento en el juego), permitiendo identificar y analizar movimientos como \"pase\" o \"regate\" sin necesidad de definirlos previamente.  Permiten segmentar el juego en secuencias de situaciones aprendidas de forma no supervisada y predecir el futuro cercano de cada situaci칩n.'\n",
    "\n",
    "    - Maximal marginal relevance: 'El texto proporcionado no define expl칤citamente qu칠 es un \"conceptor\" en el contexto del an치lisis de juegos de f칰tbol.  El t칤tulo menciona \"Clustering and Conceptors\", sugiriendo que son una herramienta o t칠cnica utilizada junto con el clustering (agrupamiento) para analizar partidos de f칰tbol, pero no se ofrece una explicaci칩n de su funci칩n o significado.'\n",
    "\n",
    "    - Similarity Score Threshold (score_threshold = 0.5): En el contexto del an치lisis de juegos de f칰tbol, un \"conceptor\" se asocia con diferentes movimientos dentro del juego.  El texto no define expl칤citamente qu칠 *es* un concepter, pero implica que son unidades o categor칤as que representan acciones o secuencias de acciones dentro de un partido.  El objetivo del estudio es identificar estos \"conceptos\" a partir de datos, utilizando m칠todos de clustering para agrupar movimientos y establecer un repertorio de los mismos que conforman el juego completo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENJiPPM0giX8"
   },
   "source": [
    "### **2.2 Agentes (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/rcqnN2aJCSEAAAAd/secret-agent-man.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la secci칩n anterior, en esta secci칩n se busca habilitar **Agentes** para obtener informaci칩n a trav칠s de tools y as칤 responder la pregunta del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V47l7Mjfrk0N"
   },
   "source": [
    "#### **2.2.1 Tool de Tavily (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas al motor de b칰squeda **Tavily**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6SLKwcWr0AG"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults(max_results = 1) \n",
    "tools = [search] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SonB1A-9rtRq"
   },
   "source": [
    "#### **2.2.2 Tool de Wikipedia (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas a **Wikipedia**.\n",
    "\n",
    "*Hint: Le puede ser de ayuda el siguiente [link](https://python.langchain.com/v0.1/docs/modules/tools/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ehJJpoqsr26-"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=100)\n",
    "tool = WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvUIMdX6r0ne"
   },
   "source": [
    "#### **2.2.3 Crear Agente (0.3 puntos)**\n",
    "\n",
    "Crear un agente que pueda responder preguntas preguntas usando las *tools* antes generadas. Aseg칰rese que su agente responda en espa침ol. Por 칰ltimo, guarde el agente en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "react_prompt = hub.pull(\"hwchase17/react\") \n",
    "print(react_prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, agent=RunnableAgent(runnable=RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_log_to_str(x['intermediate_steps']))\n",
       "})\n",
       "| PromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={}, partial_variables={'tools': \"TavilySearch(tool_input: 'Union[str, dict[str, Any]]', verbose: 'Optional[bool]' = None, start_color: 'Optional[str]' = 'green', color: 'Optional[str]' = 'green', callbacks: 'Callbacks' = None, *, tags: 'Optional[list[str]]' = None, metadata: 'Optional[dict[str, Any]]' = None, run_name: 'Optional[str]' = None, run_id: 'Optional[uuid.UUID]' = None, config: 'Optional[RunnableConfig]' = None, tool_call_id: 'Optional[str]' = None, **kwargs: 'Any') -> 'Any' - Busca resultados en Tavily.\\nWikipediaQuery(tool_input: 'Union[str, dict[str, Any]]', verbose: 'Optional[bool]' = None, start_color: 'Optional[str]' = 'green', color: 'Optional[str]' = 'green', callbacks: 'Callbacks' = None, *, tags: 'Optional[list[str]]' = None, metadata: 'Optional[dict[str, Any]]' = None, run_name: 'Optional[str]' = None, run_id: 'Optional[uuid.UUID]' = None, config: 'Optional[RunnableConfig]' = None, tool_call_id: 'Optional[str]' = None, **kwargs: 'Any') -> 'Any' - Consulta Wikipedia.\", 'tool_names': 'TavilySearch, WikipediaQuery'}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'react', 'lc_hub_commit_hash': 'd15fe3c426f1c4b3f37c9198853e4a86e20c425ca7f4752ec0c9b0e97ca7ea4d'}, template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}')\n",
       "| RunnableBinding(bound=ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), temperature=0.0, max_retries=2, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x1452bd8d0>, default_metadata=()), kwargs={'stop': ['\\nObservation']}, config={}, config_factories=[])\n",
       "| ReActSingleInputOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[Tool(name='TavilySearch', description='Busca resultados en Tavily.', func=<bound method BaseTool.run of TavilySearchResults(max_results=1, api_wrapper=TavilySearchAPIWrapper(tavily_api_key=SecretStr('**********')))>), Tool(name='WikipediaQuery', description='Consulta Wikipedia.', func=<bound method BaseTool.run of WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/opt/homebrew/lib/python3.11/site-packages/wikipedia/__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=100))>)])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain.agents import Tool\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"TavilySearch\", func=search.run, description=\"Busca resultados en Tavily.\"),\n",
    "    Tool(name=\"WikipediaQuery\", func=tool.run, description=\"Consulta Wikipedia.\")\n",
    "]\n",
    "\n",
    "agent = create_react_agent(llm, tools, react_prompt) \n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) \n",
    "agent_executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKV0JxK3r-XG"
   },
   "source": [
    "#### **2.2.4 Verificaci칩n de respuestas (0.3 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente y aseg칰rese que el agente est칠 ocupando correctamente las tools disponibles. 쮼n qu칠 casos el agente deber칤a ocupar la tool de Tavily? 쮼n qu칠 casos deber칤a ocupar la tool de Wikipedia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Pqo2dsxvywW_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To answer this question, I need to find information about the winner of the 2022 FIFA World Cup.  Wikipedia is a reliable source for this type of information.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"2022 FIFA World Cup winner\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: qu칠 equipo gan칩 el mundial de f칰tbol 2022?\n",
      "Thought:To answer this question, I need to find information about the winner of the 2022 FIFA World Cup.  Wikipedia is a reliable source for this type of information.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"2022 FIFA World Cup winner\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: qu칠 equipo gan칩 el mundial de f칰tbol 2022?\n",
      "Thought:To answer this question, I need to find information about the winner of the 2022 FIFA World Cup.  Wikipedia is a reliable source for this type of information.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"2022 FIFA World Cup winner\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: qu칠 equipo gan칩 el mundial de f칰tbol 2022?\n",
      "Thought:To answer this question, I need to find information about the winner of the 2022 FIFA World Cup.  Wikipedia is a reliable source for this type of information.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"2022 FIFA World Cup winner\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: qu칠 equipo gan칩 el mundial de f칰tbol 2022?\n",
      "Thought:To answer this question, I need to find information about the winner of the 2022 FIFA World Cup.  Wikipedia is a reliable source for this type of information.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"2022 FIFA World Cup winner\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mThought:To answer this question, I need to find information about the winner of the 2022 FIFA World Cup.  Wikipedia is a reliable source for this type of information.  The previous attempts only showed the summary, I need to extract the winner from the page content.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"2022 FIFA World Cup winner\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: qu칠 equipo gan칩 el mundial de f칰tbol 2022?\n",
      "Thought:To answer this question, I need to find information about the winner of the 2022 FIFA World Cup.  Wikipedia is a reliable source for this type of information.  I need to ensure I get the full page content, not just the summary, to find the winner.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"2022 FIFA World Cup\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mThought: The previous attempts only returned the summary. I need to find a way to access the full Wikipedia page content to extract the winner.  I'll try a different approach.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: {\"query\": \"2022 FIFA World Cup\", \"extract\": True}\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to use a more specific query to get the winner of the 2022 FIFA World Cup from Wikipedia.  The previous attempts failed to extract the relevant information.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: {\"query\": \"2022 FIFA World Cup\", \"extract\": True}\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to find a way to get the full text of the Wikipedia article for the 2022 FIFA World Cup.  The previous attempts using the `extract` parameter didn't work.  I'll try a different approach, focusing on getting the full page content and then processing it to find the winner.  I'll assume that the winner is explicitly mentioned in the article.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"2022 FIFA World Cup\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to find a way to get the full text of the Wikipedia article for the 2022 FIFA World Cup.  The previous attempts using the `extract` parameter didn't work, and only returned summaries.  I'll try a different approach, focusing on getting the full page content and then processing it to find the winner.  I'll assume that the winner is explicitly mentioned in the article.  I will need to use a different tool or method to extract the winner from the full text.  Since I don't have text processing capabilities, I will rely on the Wikipedia page being well-structured enough that a simple search for \"winner\" will suffice.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"2022 FIFA World Cup\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mThought: The previous attempts to extract the winner from the Wikipedia page have failed.  The provided tools don't seem to offer robust text processing capabilities to extract the answer from the full page content.  I need a different approach.  I will try a simpler query directly asking for the winner.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Who won the 2022 FIFA World Cup?\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mThought: The previous attempts have failed to extract the winner from the Wikipedia page.  The tools provided lack the ability to process the full page text effectively.  I will try a different search engine that might provide a more concise answer.  However, since I only have access to WikipediaQuery and TavilySearch, and neither are designed for this type of text extraction, I will attempt one last Wikipedia query with a more direct question.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Argentina 2022 FIFA World Cup\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup final\n",
      "Summary: The 2022 FIFA World Cup final was the final match of the 20\u001b[0m\u001b[32;1m\u001b[1;3mThought: I will try a final Wikipedia query, focusing on the final match to find the winner.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"2022 FIFA World Cup Final Winner\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup final\n",
      "Summary: The 2022 FIFA World Cup final was the final match of the 20\u001b[0m\u001b[32;1m\u001b[1;3mThought: I have tried various queries on Wikipedia, but haven't been able to extract the winner reliably. The tools provided lack robust text processing capabilities.  I will need to assume that a simple search for \"winner\" within the Wikipedia article would have worked if the tools allowed for that.  Based on my general knowledge, I know Argentina won.\n",
      "\n",
      "Final Answer: Argentina\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Argentina\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"qu칠 equipo gan칩 el mundial de f칰tbol 2022?\"})\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Para responder a la pregunta \"쮺u치l es el mejor restaurante en Santiago?\", necesito buscar informaci칩n sobre restaurantes en Santiago y sus rese침as.  No tengo acceso a informaci칩n en tiempo real, incluyendo rese침as de restaurantes, por lo que usar TavilySearch podr칤a ser 칰til si tiene informaci칩n sobre rankings o rese침as de restaurantes.  WikipediaQuery probablemente no sea tan 칰til para esta pregunta espec칤fica, ya que Wikipedia no suele tener rankings subjetivos de restaurantes.\n",
      "\n",
      "Action: TavilySearch\n",
      "Action Input: \"mejores restaurantes Santiago Chile\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://santiagosecreto.com/mejores-restaurantes-santiago/', 'content': 'Es por eso que lidera la lista de los mejores restaurantes de Santiago. Nueva de Lyon 99, Providencia, Santiago de Chile. Lunes a s치bado 13:00 a 23:00 hrs. 2. La Vinoteca. Santiago Secreto. La Vinoteca es tienda, restaurante y una charcuter칤a que te entrega una experiencia completa a trav칠s de la historia del vino.'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: La observaci칩n de TavilySearch proporciona un enlace a una lista de los mejores restaurantes de Santiago seg칰n santiagosecreto.com.  Para obtener una respuesta m치s completa, deber칤a acceder al enlace y extraer la informaci칩n relevante. Sin embargo, como no puedo acceder a internet, solo puedo usar la informaci칩n proporcionada en la observaci칩n.\n",
      "\n",
      "Thought: I now know the final answer.  Based on the limited information, one of the best restaurants in Santiago, according to santiagosecreto.com, is mentioned as \"Es por eso que lidera la lista de los mejores restaurantes de Santiago\".  The specific name isn't provided in the snippet, but the website is cited as the source.\n",
      "\n",
      "Final Answer: Seg칰n el sitio web santiagosecreto.com, uno de los mejores restaurantes de Santiago es mencionado como el que \"lidera la lista de los mejores restaurantes de Santiago\", aunque el nombre espec칤fico no se proporciona en el fragmento de texto.  Para una lista m치s completa, se debe consultar el enlace proporcionado: https://santiagosecreto.com/mejores-restaurantes-santiago/\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Seg칰n el sitio web santiagosecreto.com, uno de los mejores restaurantes de Santiago es mencionado como el que \"lidera la lista de los mejores restaurantes de Santiago\", aunque el nombre espec칤fico no se proporciona en el fragmento de texto.  Para una lista m치s completa, se debe consultar el enlace proporcionado: https://santiagosecreto.com/mejores-restaurantes-santiago/\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"쮺u치l es el mejor restaurante en Santiago?\"})\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se ve en las respuestas, usa Wikipedia para respuestas que tienen m치s contexto o que pueden ser respondidas a partir de informaci칩n documentada, en cambio usa Tavily cuando la respuesta no es tan directa o cuando hay menos informaci칩n objetiva del tema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZbDTYiogquv"
   },
   "source": [
    "### **2.3 Multi Agente (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/r7QMJLxU4BoAAAAd/this-is-getting-out-of-hand-star-wars.gif\"\n",
    "\" width=\"450\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsecci칩n es encapsular las funcionalidades creadas en una soluci칩n multiagente con un **supervisor**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-iUfH0WvI6m"
   },
   "source": [
    "#### **2.3.1 Generando Tools (0.5 puntos)**\n",
    "\n",
    "Transforme la soluci칩n RAG de la secci칩n 2.1 y el agente de la secci칩n 2.2 a *tools* (una tool por cada uno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_tool_function(query: str) -> str:\n",
    "    try:\n",
    "        respuesta = agent_executor.invoke({\"input\": query})\n",
    "        return respuesta[\"output\"]  \n",
    "    except Exception as e:\n",
    "        return f\"Error en AgentTool: {e}\"\n",
    "\n",
    "def rag_tool_function(query: str) -> str:\n",
    "    try:\n",
    "        respuesta = rag_chain.run({\"query\": query}) \n",
    "        return respuesta\n",
    "    except Exception as e:\n",
    "        return f\"Error en RAGTool: {e}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQYNjT_0vPCg"
   },
   "source": [
    "#### **2.3.2 Agente Supervisor (0.5 puntos)**\n",
    "\n",
    "Habilite un agente que tenga acceso a las tools del punto anterior y pueda responder preguntas relacionadas. Almacene este agente en una variable llamada supervisor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain.agents import Tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"RAGTool\",\n",
    "        func=rag_tool_function,\n",
    "        description=\"Utiliza la soluci칩n RAG para responder preguntas basadas en documentos PDF.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"AgentTool\",\n",
    "        func=agent_tool_function,\n",
    "        description=\"Un agente que utiliza Wikipedia y Tavily para responder preguntas.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "supervisor = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea3zWlvyvY7K"
   },
   "source": [
    "#### **2.3.3 Verificaci칩n de respuestas (0.25 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente repitiendo las preguntas realizadas en las secciones 2.1.4 y 2.2.4 y comente sus resultados. 쮺칩mo var칤an las respuestas bajo este enfoque?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Necesito acceder al contenido del documento sobre Expected Goals (xG) para determinar su objetivo principal.  Como no tengo acceso directo a archivos PDF, usar칠 AgentTool para buscar informaci칩n sobre el objetivo principal de los modelos xG.\n",
      "\n",
      "Action: AgentTool\n",
      "Action Input: \"쮺u치l es el objetivo principal de los modelos de Expected Goals (xG) en el f칰tbol?\"\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Para responder a la pregunta sobre el objetivo principal de los modelos xG en el f칰tbol, necesito buscar informaci칩n sobre estos modelos.  Wikipedia ser칤a una buena fuente para obtener una definici칩n general y su prop칩sito.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Expected Goals (xG) in football\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Expected goals\n",
      "Summary: In association football, expected goals (xG) is a performance metric u\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: 쮺u치l es el objetivo principal de los modelos de Expected Goals (xG) en el f칰tbol?\"\n",
      "Thought: Para responder a la pregunta sobre el objetivo principal de los modelos xG en el f칰tbol, necesito buscar informaci칩n sobre estos modelos.  Wikipedia ser칤a una buena fuente para obtener una definici칩n general y su prop칩sito.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Expected Goals (xG) in football\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Expected goals\n",
      "Summary: In association football, expected goals (xG) is a performance metric u\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: 쮺u치l es el objetivo principal de los modelos de Expected Goals (xG) en el f칰tbol?\"\n",
      "Thought: Para responder a la pregunta sobre el objetivo principal de los modelos xG en el f칰tbol, necesito buscar informaci칩n sobre estos modelos. Wikipedia ser칤a una buena fuente para obtener una definici칩n general y su prop칩sito.\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Expected Goals (xG) in football\"\u001b[0m\u001b[33;1m\u001b[1;3mPage: Expected goals\n",
      "Summary: In association football, expected goals (xG) is a performance metric u\u001b[0m\u001b[32;1m\u001b[1;3mThought: The previous attempts only provided a partial summary. I need to retrieve the full Wikipedia article to understand the main objective of xG models in football.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Expected Goals (xG) in football\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Expected goals\n",
      "Summary: In association football, expected goals (xG) is a performance metric u\u001b[0m\u001b[32;1m\u001b[1;3mThought: The previous attempts only provided a partial summary. I need to retrieve the full Wikipedia article to understand the main objective of xG models in football.  I will try a different approach, focusing on the *objective* of xG models.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Objective of Expected Goals (xG) in football\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Glossary of association football terms\n",
      "Summary: Association football (more commonly known as f\u001b[0m\u001b[32;1m\u001b[1;3mThought: The Wikipedia queries haven't provided a direct answer to the main objective of xG models.  I'll try a more specific query focusing on the purpose.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Purpose of Expected Goals (xG) in football\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Glossary of Australian rules football\n",
      "Summary: This list is an alphabetical glossary of Austra\u001b[0m\u001b[32;1m\u001b[1;3mThought: The Wikipedia queries are not yielding a satisfactory answer.  I will try a different approach using a broader search engine like TavilySearch, hoping to find articles or resources that explicitly state the main objective of xG models.\n",
      "\n",
      "Action: TavilySearch\n",
      "Action Input: \"main objective expected goals xG football\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://footballxg.com/what_are_expected_goals/', 'content': 'Expected goals (xG) is a metric that calculates how many goals a team should have scored based on the quality of the chances created. Learn how xG is calculated, how it can predict match outcomes, and its limitations and advantages.'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: The TavilySearch result provides a concise explanation of xG.  I can now formulate the final answer.\n",
      "\n",
      "Final Answer: El objetivo principal de los modelos de Expected Goals (xG) en el f칰tbol es calcular cu치ntos goles deber칤a haber marcado un equipo bas치ndose en la calidad de las ocasiones de gol creadas.  Esto permite evaluar el rendimiento ofensivo de un equipo m치s all치 del simple n칰mero de goles anotados, considerando la calidad de las oportunidades generadas.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mEl objetivo principal de los modelos de Expected Goals (xG) en el f칰tbol es calcular cu치ntos goles deber칤a haber marcado un equipo bas치ndose en la calidad de las ocasiones de gol creadas.  Esto permite evaluar el rendimiento ofensivo de un equipo m치s all치 del simple n칰mero de goles anotados, considerando la calidad de las oportunidades generadas.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: La informaci칩n obtenida de AgentTool proporciona una respuesta completa a la pregunta. No necesito m치s acciones.\n",
      "\n",
      "Thought: I now know the final answer\n",
      "Final Answer: El objetivo principal de los modelos de Expected Goals (xG) es calcular cu치ntos goles deber칤a haber marcado un equipo bas치ndose en la calidad de las ocasiones de gol creadas, ofreciendo una evaluaci칩n del rendimiento ofensivo m치s all치 del simple conteo de goles anotados.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "El objetivo principal de los modelos de Expected Goals (xG) es calcular cu치ntos goles deber칤a haber marcado un equipo bas치ndose en la calidad de las ocasiones de gol creadas, ofreciendo una evaluaci칩n del rendimiento ofensivo m치s all치 del simple conteo de goles anotados.\n"
     ]
    }
   ],
   "source": [
    "respuesta1 = supervisor.run(\"쮺u치l es el objetivo principal del documento sobre Expected Goals (xG)?\")\n",
    "print(respuesta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Necesito acceder al documento de clustering de jugadores de f칰tbol para identificar los m칠todos utilizados.  Como no tengo acceso a ese documento espec칤fico, debo usar una herramienta que pueda acceder a informaci칩n general sobre m칠todos de clustering usados en an치lisis deportivos.  AgentTool parece la mejor opci칩n para esto.\n",
      "\n",
      "Action: AgentTool\n",
      "Action Input: \"쯈u칠 m칠todos de clustering se utilizan com칰nmente en el an치lisis de datos de jugadores de f칰tbol?\"\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Para responder a la pregunta sobre qu칠 m칠todos de clustering se utilizan com칰nmente en el an치lisis de datos de jugadores de f칰tbol, necesito buscar informaci칩n sobre t칠cnicas de clustering aplicadas en el an치lisis deportivo.  Tanto TavilySearch como WikipediaQuery podr칤an ser 칰tiles, pero WikipediaQuery podr칤a ser m치s general y necesitar칤a una b칰squeda m치s espec칤fica en TavilySearch si no encuentro suficiente informaci칩n.  Intentar칠 primero con Wikipedia, buscando informaci칩n sobre \"clustering\" y \"an치lisis deportivo\" o \"an치lisis de rendimiento deportivo\". Si no obtengo resultados satisfactorios, usar칠 TavilySearch con una consulta m치s espec칤fica.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"clustering\" AND (\"sports analytics\" OR \"sports performance analysis\")\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Network Science Based Basketball Analytics\n",
      "Summary: Network Science based basketball analytics\u001b[0m\u001b[32;1m\u001b[1;3mThought: La observaci칩n de Wikipedia no proporciona directamente los m칠todos de clustering usados en el an치lisis de datos de jugadores de f칰tbol. Necesito una b칰squeda m치s espec칤fica y enfocada en los m칠todos de clustering.  Usar칠 TavilySearch para obtener informaci칩n m치s precisa.\n",
      "\n",
      "Action: TavilySearch\n",
      "Action Input: \"clustering methods in soccer player data analysis\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://arxiv.org/pdf/2204.09793', 'content': \"Clustering of football players performance data Akhanli and Hennig data. Cluster analysis has been used for aggregating similar types of players in sev-eral applications.Ogles and Masters(2003) suggested that by using cluster analysis (Ward's method), marathon runners can be categorised into 遼ve groups in terms of\"}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: La respuesta de TavilySearch proporciona un ejemplo del m칠todo de clustering de Ward.  Aunque solo menciona un m칠todo, es un buen comienzo.  Para obtener una respuesta m치s completa, buscar칠 m치s informaci칩n en TavilySearch usando t칠rminos m치s generales.\n",
      "\n",
      "Action: TavilySearch\n",
      "Action Input: \"clustering methods sports analytics\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.sciencedirect.com/science/article/pii/S2405844024093289', 'content': \"The rapid advancement of sports science and data analytics underscores the critical need for a sophisticated analytical approach. ... A highly intelligent clustering method of athletes' sports behavior characteristics, grounded in advanced algorithms is proposed to explore the correlation between the behavior characteristics of athletes and\"}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: La informaci칩n obtenida de TavilySearch menciona el m칠todo de Ward y sugiere la existencia de otros m칠todos de clustering inteligentes en el an치lisis deportivo, pero no los especifica.  Para obtener una lista m치s completa de m칠todos com칰nmente usados, necesitar칠 buscar en otras fuentes o refinar mis b칰squedas en TavilySearch.  Sin embargo, con la informaci칩n actual, puedo dar una respuesta parcial.\n",
      "\n",
      "Final Answer:  Basado en la informaci칩n disponible, el m칠todo de clustering de Ward se ha utilizado en el an치lisis de datos de jugadores de f칰tbol.  Otras t칠cnicas de clustering \"inteligentes\" tambi칠n se emplean en el an치lisis deportivo, aunque no se especifican en las fuentes consultadas.  Se requiere una investigaci칩n m치s exhaustiva para obtener una lista completa de los m칠todos com칰nmente utilizados.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mBasado en la informaci칩n disponible, el m칠todo de clustering de Ward se ha utilizado en el an치lisis de datos de jugadores de f칰tbol.  Otras t칠cnicas de clustering \"inteligentes\" tambi칠n se emplean en el an치lisis deportivo, aunque no se especifican en las fuentes consultadas.  Se requiere una investigaci칩n m치s exhaustiva para obtener una lista completa de los m칠todos com칰nmente utilizados.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: La respuesta de AgentTool es parcial.  Necesitar칤a m치s informaci칩n para dar una respuesta completa.  Sin embargo, la respuesta indica que al menos un m칠todo (Ward) es usado.  Puedo intentar refinar la pregunta a AgentTool para obtener m치s detalles.\n",
      "\n",
      "Action: AgentTool\n",
      "Action Input: \"쯈u칠 algoritmos de clustering, adem치s del m칠todo de Ward, se utilizan en el an치lisis de rendimiento de jugadores de f칰tbol?  Menciona ejemplos concretos si es posible.\"\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Para responder a la pregunta sobre algoritmos de clustering usados en el an치lisis de rendimiento de jugadores de f칰tbol, adem치s del m칠todo de Ward, necesito buscar informaci칩n en una base de datos o enciclopedia que trate temas deportivos y an치lisis de datos.  Wikipedia podr칤a tener informaci칩n general sobre algoritmos de clustering, pero es menos probable que tenga ejemplos espec칤ficos de su aplicaci칩n en el an치lisis de rendimiento deportivo.  Una b칰squeda en Tavily (asumiendo que es una base de datos o motor de b칰squeda similar a Google Scholar) podr칤a ser m치s fruct칤fera.  Intentar칠 primero con Tavily, y si no obtengo resultados satisfactorios, recurrir칤a a Wikipedia.\n",
      "\n",
      "Action: TavilySearch\n",
      "Action Input: \"algoritmos clustering an치lisis rendimiento jugadores f칰tbol m칠todos Ward\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://uvadoc.uva.es/handle/10324/57954', 'content': 'Uso de t칠cnicas de clustering para encontrar perfiles de jugadores en una competici칩n de f칰tbol profesional ... Resumen. El Big Data se ha ido haciendo un hueco en el mundo del f칰tbol, convirtiendose en un fijo en el personal de la mayoria de equipos, siendo fundamental en el an치lisis del rendimiento de los jugadores tanto en los partidos'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: La observaci칩n de TavilySearch proporciona un enlace a un documento que parece relevante.  Sin embargo, no especifica qu칠 algoritmos de clustering se utilizan.  Intentar칠 acceder al documento (si es posible) para obtener m치s informaci칩n. Si no puedo acceder al contenido, usar칠 Wikipedia para obtener informaci칩n general sobre algoritmos de clustering y luego intentar칠 inferir su aplicabilidad al an치lisis de rendimiento de jugadores de f칰tbol.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"clustering algorithms\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: K-means clustering\n",
      "Summary: k-means clustering is a method of vector quantization, originally \u001b[0m\u001b[32;1m\u001b[1;3mThought: La b칰squeda en Wikipedia sobre \"clustering algorithms\" proporciona informaci칩n general, pero no ejemplos espec칤ficos en el contexto del an치lisis de rendimiento de jugadores de f칰tbol.  Necesitar칠 buscar m치s informaci칩n para responder completamente a la pregunta.  Intentar칠 una b칰squeda m치s espec칤fica en Tavily.\n",
      "\n",
      "Action: TavilySearch\n",
      "Action Input: \"clustering algorithms football player performance analysis k-means hierarchical clustering DBSCAN\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://github.com/SaySohail/football-player-similarity-kmeans-gmm', 'content': 'Two clustering algorithms, K-means and Gaussian Mixture Model (GMM), were used to cluster the football players based on their performance data. 4.1 K-means K-means clustering is an algorithm that involves specifying the number of clusters (K) and iteratively assigning each data point to one of the clusters based on the similarity of their features.'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: La observaci칩n de TavilySearch ahora proporciona un enlace a un repositorio de GitHub que menciona expl칤citamente el uso de K-means y Gaussian Mixture Model (GMM) para el clustering de jugadores de f칰tbol basados en su rendimiento.  Esto responde a la pregunta.\n",
      "\n",
      "Final Answer: Adem치s del m칠todo de Ward, algoritmos como K-means y Gaussian Mixture Model (GMM) se utilizan en el an치lisis de rendimiento de jugadores de f칰tbol.  Un ejemplo concreto es el uso de K-means y GMM para agrupar jugadores seg칰n sus datos de rendimiento, como se muestra en un repositorio de GitHub (enlace proporcionado en la observaci칩n anterior).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mAdem치s del m칠todo de Ward, algoritmos como K-means y Gaussian Mixture Model (GMM) se utilizan en el an치lisis de rendimiento de jugadores de f칰tbol.  Un ejemplo concreto es el uso de K-means y GMM para agrupar jugadores seg칰n sus datos de rendimiento, como se muestra en un repositorio de GitHub (enlace proporcionado en la observaci칩n anterior).\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: La respuesta de AgentTool ahora proporciona una lista m치s completa de m칠todos de clustering.  Puedo usar esta informaci칩n para formular mi respuesta final.\n",
      "\n",
      "Thought: I now know the final answer\n",
      "Final Answer:  Basado en la informaci칩n disponible, los m칠todos de clustering com칰nmente utilizados en el an치lisis de datos de jugadores de f칰tbol incluyen el m칠todo de Ward, K-means y Gaussian Mixture Model (GMM).  Es posible que se utilicen otros m칠todos, pero estos tres son ejemplos concretos.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Basado en la informaci칩n disponible, los m칠todos de clustering com칰nmente utilizados en el an치lisis de datos de jugadores de f칰tbol incluyen el m칠todo de Ward, K-means y Gaussian Mixture Model (GMM).  Es posible que se utilicen otros m칠todos, pero estos tres son ejemplos concretos.\n"
     ]
    }
   ],
   "source": [
    "respuesta2 = supervisor.run(\"쯈u칠 m칠todos de clustering se utilizan en el documento de clustering de jugadores de f칰tbol?\")\n",
    "print(respuesta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: No tengo conocimiento previo sobre el t칠rmino \"conceptor\" en el an치lisis de juegos de f칰tbol.  Intentar칠 usar AgentTool para buscar informaci칩n en Wikipedia y Tavily.\n",
      "\n",
      "Action: AgentTool\n",
      "Action Input: \"쯈u칠 es un 'conceptor' en el contexto del an치lisis de juegos de f칰tbol?\"\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Necesito encontrar informaci칩n sobre el t칠rmino \"conceptor\" en el contexto del an치lisis de juegos de f칰tbol.  Como es un t칠rmino espec칤fico del deporte, es probable que Wikipedia no tenga una entrada dedicada.  Intentar칠 primero con una b칰squeda en Tavily, que podr칤a tener informaci칩n de blogs, foros o sitios web especializados en an치lisis futbol칤stico.\n",
      "\n",
      "Action: TavilySearch\n",
      "Action Input: \"conceptor an치lisis juego f칰tbol\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://futboldelibro.com/wp-content/uploads/2019/10/MuestraModeloDeJuego.pdf', 'content': 'MODELO DE JUEGO: ESTRUCTURA, METODOLOG칈A Y APLICACI칍N PR츼CTICA. ... cursos y sabiendo la evoluci칩n y revoluci칩n que ha tenido el f칰tbol a nivel de conceptos y nuevas formas de entrenamientos y entendimientos; es de admirar la ... del an치lisis del rival, que puede modificar aspectos de nuestro juego en momentos'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: La observaci칩n de TavilySearch proporciona un PDF que parece tratar sobre modelos de juego en f칰tbol, pero no define expl칤citamente \"conceptor\".  Necesitar칠 m치s informaci칩n. Intentar칠 una b칰squeda m치s espec칤fica en Tavily, incluyendo sin칩nimos o t칠rminos relacionados.\n",
      "\n",
      "Action: TavilySearch\n",
      "Action Input: \"jugador creador juego f칰tbol an치lisis t치ctico\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://quicklineup.com/es', 'content': 'Usa nuestro creador de plantilla de f칰tbol para formar tu equipo de 11 jugadores. Crea, personaliza y comparte tu alineaci칩n de f칰tbol ideal con Quick Lineup. ... El f칰tbol no es solo un juego de once jugadores, sino tambi칠n una danza de t치cticas, estrategias e imaginaci칩n. ... An치lisis T치ctico 춰Tienes muchas opciones para probar'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: La segunda b칰squeda en Tavily tampoco proporciona una definici칩n directa de \"conceptor\".  Las respuestas parecen apuntar a recursos relacionados con el an치lisis t치ctico y la creaci칩n de plantillas, pero no definen el t칠rmino.  Intentar칠 una b칰squeda en Wikipedia sobre conceptos relacionados con el an치lisis t치ctico del f칰tbol, esperando encontrar alguna referencia indirecta.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"An치lisis t치ctico f칰tbol\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Liga Nacional de F칰tbol Profesional\n",
      "Summary: The Liga Nacional de F칰tbol Profesional (transl.겦u001b[0m\u001b[32;1m\u001b[1;3mThought: La b칰squeda en Wikipedia no arroj칩 una definici칩n directa de \"conceptor\" en el contexto del an치lisis de f칰tbol.  La informaci칩n obtenida de Tavily tampoco fue 칰til.  Es probable que \"conceptor\" sea un t칠rmino menos com칰n o utilizado en un contexto espec칤fico, quiz치s dentro de una escuela de pensamiento particular sobre el an치lisis t치ctico del f칰tbol.  Sin m치s informaci칩n, no puedo definir el t칠rmino.\n",
      "\n",
      "Final Answer: No se encontr칩 una definici칩n de \"conceptor\" en el contexto del an치lisis de juegos de f칰tbol utilizando las herramientas disponibles.  Es posible que el t칠rmino sea poco com칰n o espec칤fico de un 치mbito particular del an치lisis t치ctico.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mNo se encontr칩 una definici칩n de \"conceptor\" en el contexto del an치lisis de juegos de f칰tbol utilizando las herramientas disponibles.  Es posible que el t칠rmino sea poco com칰n o espec칤fico de un 치mbito particular del an치lisis t치ctico.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: La b칰squeda en AgentTool no tuvo 칠xito.  Podr칤a ser que el t칠rmino sea muy espec칤fico o un neologismo. Intentar칠 reformular la pregunta para ver si obtengo mejores resultados.\n",
      "\n",
      "Action: AgentTool\n",
      "Action Input: \"Definir 'conceptor' en el contexto del f칰tbol.  Si no existe una definici칩n espec칤fica, describir roles de jugadores de f칰tbol que podr칤an ser considerados 'conceptores' en un an치lisis t치ctico.\"\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: No existe una definici칩n est치ndar de \"conceptor\" en el contexto del f칰tbol.  Deber칤a buscar en Wikipedia informaci칩n sobre roles de jugadores de f칰tbol y luego intentar identificar cu치les podr칤an considerarse \"conceptores\" bas치ndome en un an치lisis t치ctico.  Primero buscar칠 en Wikipedia para ver si hay alguna menci칩n, aunque sea indirecta.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"roles de jugadores de f칰tbol\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Liga Nacional de F칰tbol Profesional\n",
      "Summary: The Liga Nacional de F칰tbol Profesional (transl.겦u001b[0m\u001b[32;1m\u001b[1;3mThought:The Wikipedia query about \"roles de jugadores de f칰tbol\" returned a page about a specific football league, not a general overview of player roles.  I need a more general search to find information about different player positions and their tactical functions.  I will try a different Wikipedia query.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"posiciones de f칰tbol\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2011 Liga de F칰tbol Profesional Boliviano season\n",
      "Summary: The 2011 Liga de F칰tbol Profesional \u001b[0m\u001b[32;1m\u001b[1;3mThought:The Wikipedia query about \"posiciones de f칰tbol\" also returned a page about a specific football league, not a general overview of player positions.  I need to try a different approach.  I will try searching for specific player roles and then infer which might fit the \"conceptor\" description.  Since \"conceptor\" implies a player who creates plays, I'll look for roles associated with playmaking.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"playmaker f칰tbol\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Envigado F.C.\n",
      "Summary: Envigado F칰tbol Club (Spanish pronunciation: [embi틛톢a칧o 틛fu칧쑖l 틛klu쑇)\u001b[0m\u001b[32;1m\u001b[1;3mThought:The Wikipedia query about \"playmaker f칰tbol\" returned a page about a specific football club, not information about the playmaker role.  I will try a different search term.  I will use a more general search on Wikipedia to find information about different attacking midfield roles.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"mediocampista ofensivo f칰tbol\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Mariano Mi침o\n",
      "Summary: Alan Mariano Mi침o (born March 28, 1994) is an Argentinian footballer who\u001b[0m\u001b[32;1m\u001b[1;3mThought:The Wikipedia queries haven't yielded a satisfactory definition of \"conceptor\" in football or a list of player roles that directly use that term.  However, the searches have highlighted the difficulty of finding a precise definition.  I will now attempt to infer potential roles based on the implied meaning of \"conceptor,\" which suggests a player who conceives or creates attacking plays.\n",
      "\n",
      "Thought: I will now describe roles that could be considered \"conceptores\" based on their tactical functions.\n",
      "\n",
      "Thought: I now know the final answer.\n",
      "\n",
      "Final Answer: No existe una definici칩n formal de \"conceptor\" en el f칰tbol. Sin embargo, bas치ndonos en la connotaci칩n de crear o concebir jugadas, varios roles podr칤an considerarse \"conceptores\" en un an치lisis t치ctico.  Estos incluyen, pero no se limitan a:\n",
      "\n",
      "* **Mediocampista ofensivo/creativo:** Este jugador es el cerebro del equipo, dictando el ritmo del juego y creando oportunidades de gol a trav칠s de pases precisos, visi칩n de juego y habilidad t칠cnica.  Su funci칩n principal es la concepci칩n de las jugadas ofensivas.\n",
      "\n",
      "* **Mediapunta:**  Similar al mediocampista ofensivo, pero a menudo con una posici칩n m치s adelantada,  la media punta se encarga de conectar el mediocampo con la delantera, creando y finalizando jugadas.\n",
      "\n",
      "* **Delantero falso (False 9):**  Aunque nominalmente un delantero, este jugador se mueve con libertad por todo el frente de ataque, creando espacios y oportunidades para sus compa침eros a trav칠s de su movilidad y visi칩n de juego.  Su funci칩n es menos la finalizaci칩n y m치s la creaci칩n de jugadas.\n",
      "\n",
      "* **Lateral ofensivo:** En sistemas modernos, los laterales ofensivos participan activamente en la creaci칩n de juego, subiendo por la banda y proporcionando centros y pases clave.  Su contribuci칩n a la concepci칩n de jugadas es significativa.\n",
      "\n",
      "\n",
      "La designaci칩n de un jugador como \"conceptor\" depender치 del sistema t치ctico del equipo y del rol espec칤fico que ese jugador desempe침a dentro de 칠l.  No es un t칠rmino formal, sino m치s bien una interpretaci칩n anal칤tica.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mNo existe una definici칩n formal de \"conceptor\" en el f칰tbol. Sin embargo, bas치ndonos en la connotaci칩n de crear o concebir jugadas, varios roles podr칤an considerarse \"conceptores\" en un an치lisis t치ctico.  Estos incluyen, pero no se limitan a:\n",
      "\n",
      "* **Mediocampista ofensivo/creativo:** Este jugador es el cerebro del equipo, dictando el ritmo del juego y creando oportunidades de gol a trav칠s de pases precisos, visi칩n de juego y habilidad t칠cnica.  Su funci칩n principal es la concepci칩n de las jugadas ofensivas.\n",
      "\n",
      "* **Mediapunta:**  Similar al mediocampista ofensivo, pero a menudo con una posici칩n m치s adelantada,  la media punta se encarga de conectar el mediocampo con la delantera, creando y finalizando jugadas.\n",
      "\n",
      "* **Delantero falso (False 9):**  Aunque nominalmente un delantero, este jugador se mueve con libertad por todo el frente de ataque, creando espacios y oportunidades para sus compa침eros a trav칠s de su movilidad y visi칩n de juego.  Su funci칩n es menos la finalizaci칩n y m치s la creaci칩n de jugadas.\n",
      "\n",
      "* **Lateral ofensivo:** En sistemas modernos, los laterales ofensivos participan activamente en la creaci칩n de juego, subiendo por la banda y proporcionando centros y pases clave.  Su contribuci칩n a la concepci칩n de jugadas es significativa.\n",
      "\n",
      "\n",
      "La designaci칩n de un jugador como \"conceptor\" depender치 del sistema t치ctico del equipo y del rol espec칤fico que ese jugador desempe침a dentro de 칠l.  No es un t칠rmino formal, sino m치s bien una interpretaci칩n anal칤tica.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer.  The AgentTool provided a useful response, even though \"conceptor\" isn't a formally defined term in football analysis.\n",
      "\n",
      "Final Answer: No existe una definici칩n formal de \"conceptor\" en el an치lisis de f칰tbol. Sin embargo, el t칠rmino se refiere a jugadores que crean y conciben jugadas ofensivas.  Esto podr칤a incluir mediocampistas ofensivos/creativos, mediapuntas, delanteros falsos y laterales ofensivos, dependiendo del sistema t치ctico del equipo y del rol espec칤fico del jugador.  La designaci칩n de \"conceptor\" es una interpretaci칩n anal칤tica, no un t칠rmino t칠cnico establecido.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "No existe una definici칩n formal de \"conceptor\" en el an치lisis de f칰tbol. Sin embargo, el t칠rmino se refiere a jugadores que crean y conciben jugadas ofensivas.  Esto podr칤a incluir mediocampistas ofensivos/creativos, mediapuntas, delanteros falsos y laterales ofensivos, dependiendo del sistema t치ctico del equipo y del rol espec칤fico del jugador.  La designaci칩n de \"conceptor\" es una interpretaci칩n anal칤tica, no un t칠rmino t칠cnico establecido.\n"
     ]
    }
   ],
   "source": [
    "respuesta3 = supervisor.run(\"쯈u칠 es un 'conceptor' en el contexto del an치lisis de juegos de f칰tbol?\")\n",
    "print(respuesta3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Necesito encontrar informaci칩n sobre el ganador de la Copa Mundial de la FIFA 2022.  Puedo usar AgentTool ya que tiene acceso a Wikipedia, que seguramente tendr치 esta informaci칩n.\n",
      "\n",
      "Action: AgentTool\n",
      "Action Input: 쯈u칠 equipo gan칩 la Copa Mundial de la FIFA 2022?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Necesito encontrar informaci칩n sobre la Copa Mundial de la FIFA 2022 para determinar qu칠 equipo la gan칩.  Wikipedia es una buena fuente para este tipo de informaci칩n.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Copa Mundial de la FIFA 2022\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Copa Am칠rica de Futsal\n",
      "Summary: The CONMEBOL Copa Am칠rica de Futsal is the main national futsa\u001b[0m\u001b[32;1m\u001b[1;3mThought: The observation is incorrect; it's about Futsal, not the FIFA World Cup. I need to try again with a more specific query.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"2022 FIFA World Cup Winner\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mThought: The observation mentions the 2022 FIFA World Cup. I need to extract the winner from the Wikipedia page summary or content.  I'll try a more direct query.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"2022 FIFA World Cup winner\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mThought: The summary still doesn't explicitly state the winner. I need to access the full Wikipedia page content.  I'll try a different approach.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: {\"query\": \"2022 FIFA World Cup winner\", \"extract\": True}\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\u001b[32;1m\u001b[1;3mThought: The previous attempts failed to extract the winner directly.  Let's try a different approach using a more specific query to Wikipedia.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Winner of the 2022 FIFA World Cup\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mThought: The summary still doesn't give the winner. I need to look for the winner within the full Wikipedia article.  Since I'm having trouble extracting the information directly, I'll try a different strategy.  I'll search for the information using a search engine via TavilySearch.\n",
      "\n",
      "Action: TavilySearch\n",
      "Action Input: \"2022 FIFA World Cup winner\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.espn.com/soccer/match/_/gameId/633850/france-argentina', 'content': 'Game summary of the Argentina vs. France Fifa World Cup game, final score 3-3, from December 18, 2022 on ESPN.'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: The TavilySearch result mentions a game between Argentina and France.  While it doesn't explicitly state the winner, the context strongly suggests a close match.  I need to find a source that clearly states the winner.  Let's try Wikipedia again, focusing on the infobox.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: {\"query\": \"2022 FIFA World Cup\", \"extract_infobox\": True}\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\u001b[32;1m\u001b[1;3mThought: The previous attempts haven't yielded a clear winner.  Let's try a different search strategy on TavilySearch, focusing on a more concise query.\n",
      "\n",
      "Action: TavilySearch\n",
      "Action Input: \"Who won the 2022 World Cup?\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.sportingnews.com/us/soccer/news/who-won-2022-fifa-world-cup-final-score-result-highlights-qatar/aar5gfviuuapvkmcq3tf7dep', 'content': \"From the moment Lionel Messi put the Copa America holders ahead in the 23rd minute of the 2022 FIFA World Cup final against France, Lionel Scaloni's side looked determined to deny the reigning champions as they survived a late comeback and penalties to storm to a stylish triumph in the first ever winter tournament in Qatar.\\n Here's how the penalty shootout played out:\\nFIFA 2022 World Cup final highlights\\nHere's how Messi's first goal looked\\nUS viewers\\nMESSI PUTS ARGENTINA OUT IN FRONT 游꿢游뷣릖 pic.twitter.com/kZkUvRUXC9\\nCanada viewers\\nLEO MESSI PUTS ARGENTINA UP 1-0! 游뷣릖#FIFAWorldCup pic.twitter.com/xZcTiKyfWv\\nUK viewers\\nMbappe makes it 3-3 from the spot\\nUS viewers\\nMBAPPE TIES IT AGAIN3-3 IN THE 117TH MINUTE 游땸 pic.twitter.com/EelVTJMRiI\\nCanada viewers\\nMBAPPE EVENS IT UP ONCE AGAIN! #FIFAWorldCup pic.twitter.com/0NgO1CW0xI\\nUK viewers\\nMessi - Mac Allister - Di Maria 游뷣릖윥rance are caught out on a swift counter attack as Argentina take control of the game with their second goal... 游뗿#ITVFootball | #FIFAWorldCup pic.twitter.com/QCsZSYCuPL\\nThis is how Mbappe gave France hope\\nUS viewers\\nMBAPPE TAKES ONE BACK FOR FRANCE 游游윦ame. 游游#ITVFootball | #FIFAWorldCup pic.twitter.com/idYQRqrZtv\\nMessi restores Argentina's lead in extra time\\nUS viewers\\nIT HAD TO BE MESSI 游냃游뷣릖 pic.twitter.com/KF5wggitVF\\nCanada viewers\\nLEO MESSI HAS PUT ARGENTINA ON TOP!\"}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: The TavilySearch result clearly states that Argentina won the 2022 FIFA World Cup.\n",
      "\n",
      "Final Answer: Argentina\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mArgentina\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer.\n",
      "Final Answer: Argentina\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Argentina\n"
     ]
    }
   ],
   "source": [
    "response = supervisor.run(\"qu칠 equipo gan칩 el mundial de f칰tbol 2022?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para las preguntas que iban dirigidas al RAG, este enfoque entreg칩 respuestas m치s generales ya que busc칩 informaci칩n en la web, por lo que no es tan espec칤fico aplicado a los documentos como se quisiera. Para la respuesta que fue enfocada al trabajo del agente, la respuesta y el trabajo por detr치s fue casi lo mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qb8bdAmYvgwn"
   },
   "source": [
    "#### **2.3.4 An치lisis (0.25 puntos)**\n",
    "\n",
    "쯈u칠 diferencias tiene este enfoque con la soluci칩n *Router* vista en clases? Nombre al menos una ventaja y desventaja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAUlJxqoLK5r"
   },
   "source": [
    "Este enfoque como es supervisor, trabaja con los distintos llms de modo de obtener una respuesta, a diferencia del router que redigire la pregunta a un llm espec칤fico y se obtiene una sola respuesta. La ventaja de este enfoque es que obtiene respuestas m치s robustas y con m치s trabajo por detr치s, es decir, se espera que tengan m치s sentido por m치s que sean m치s resumidas en unos casos, a diferencia de la soluci칩n router que solo entrega la respuesta del llm solicitado. \n",
    "\n",
    "La desventaja de este enfoque son los tiempos de ejecuci칩n y el costo computacional que hay, ya que para algunas preguntas simples se puede tardar minutos por complejizar el trabajo detr치s utilizando varios llms, a diferencia de la soluci칩n router, que al tratar con un llm no es necesario hacer m치s trabajo por detr치s, por lo que la respuesta es m치s r치pida y m치s eficiente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JWVSuWiZ8Mj"
   },
   "source": [
    "### **2.4 Memoria (Bonus +0.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/Gs95aiElrscAAAAd/memory-unlocked-ratatouille-critic.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una de las principales falencias de las soluciones que hemos visto hasta ahora es que nuestro chat no responde las interacciones anteriores, por ejemplo:\n",
    "\n",
    "- Pregunta 1: \"Hola! mi nombre es Sebasti치n\"\n",
    "  - Respuesta esperada: \"Hola Sebasti치n! ...\"\n",
    "- Pregunta 2: \"Cual es mi nombre?\"\n",
    "  - Respuesta actual: \"Lo siento pero no conozco tu nombre :(\"\n",
    "  - **Respuesta esperada: \"Tu nombre es Sebasti치n\"**\n",
    "\n",
    "Para solucionar esto, se les solicita agregar un componente de **memoria** a la soluci칩n entregada en el punto 2.3.\n",
    "\n",
    "**Nota: El Bonus es v치lido <u>s칩lo para la secci칩n 2 de Large Language Models.</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6Y7tIPJLPfB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFc3jBT5g0kT"
   },
   "source": [
    "### **2.5 Despliegue (0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/IytHqOp52EsAAAAd/you-get-a-deploy-deploy.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una vez tengan los puntos anteriores finalizados, toca la etapa de dar a conocer lo que hicimos! Para eso, vamos a desplegar nuestro modelo a trav칠s de `gradio`, una librer칤a especializada en el levantamiento r치pido de demos basadas en ML.\n",
    "\n",
    "Primero instalamos la librer칤a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "T8TsvnCPbkIA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJBztEUovKsF"
   },
   "source": [
    "Luego s칩lo deben ejecutar el siguiente c칩digo e interactuar con la interfaz a trav칠s del notebook o del link generado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3KedQSvg1-n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* Running on public URL: https://33b3d3e55626aba88d.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://33b3d3e55626aba88d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to define \"expected goals\".  Since it's a statistical concept likely related to sports, I'll try AgentTool first, as it has access to Wikipedia which is a good source for definitions.\n",
      "\n",
      "Action: AgentTool\n",
      "Action Input: Define \"expected goals\" in soccer (football).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To define \"expected goals\" in soccer, I need to find a reliable source of information, such as Wikipedia or a reputable sports statistics website.  Wikipedia is a good starting point for a general definition.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Expected goals soccer\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Expected goals\n",
      "Summary: In association football, expected goals (xG) is a performance metric u\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Define \"expected goals\" in soccer (football).\n",
      "Thought:To define \"expected goals\" in soccer, I need to find a reliable source of information, such as Wikipedia or a reputable sports statistics website.  Wikipedia is a good starting point for a general definition.\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Expected goals soccer\"\u001b[0m\u001b[33;1m\u001b[1;3mPage: Expected goals\n",
      "Summary: In association football, expected goals (xG) is a performance metric u\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Define \"expected goals\" in soccer (football).\n",
      "Thought:To define \"expected goals\" in soccer, I need to find a reliable source of information, such as Wikipedia or a reputable sports statistics website.  Wikipedia is a good starting point for a general definition.\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Expected goals soccer\"\u001b[0m\u001b[33;1m\u001b[1;3mPage: Expected goals\n",
      "Summary: In association football, expected goals (xG) is a performance metric u\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Define \"expected goals\" in soccer (football).\n",
      "\n",
      "Thought:To define \"expected goals\" in soccer, I need to find a reliable source of information, such as Wikipedia.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Expected goals soccer\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Expected goals\n",
      "Summary: In association football, expected goals (xG) is a performance metric u\u001b[0m"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "def agent_response(message, history):\n",
    "  '''\n",
    "  Funci칩n para gradio, recibe mensaje e historial, devuelte la respuesta del chatbot.\n",
    "  '''\n",
    "  # get chatbot response\n",
    "  response = supervisor.run(message)\n",
    "\n",
    "  # assert\n",
    "  assert type(response) == str, \"output de route_question debe ser string\"\n",
    "\n",
    "  # \"streaming\" response\n",
    "  for i in range(len(response)):\n",
    "    time.sleep(0.015)\n",
    "    yield response[: i+1]\n",
    "\n",
    "gr.ChatInterface(\n",
    "    agent_response,\n",
    "    type=\"messages\",\n",
    "    title=\"Chatbot Clustering Football MDS7202\", # Pueden cambiar esto si lo desean\n",
    "    description=\"Hola! Soy un chatbot con informaci칩n de ML en el f칰tbol\", # tambi칠n la descripci칩n\n",
    "    theme=\"soft\",\n",
    "    ).launch(\n",
    "        share=True, # pueden compartir el link a sus amig@s para que interactuen con su chat!\n",
    "        debug = False,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
