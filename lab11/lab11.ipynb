{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyPTffTLug7i"
   },
   "source": [
    "# **Laboratorio 11: LLM y Agentes Autónomos 🤖**\n",
    "\n",
    "MDS7202: Laboratorio de Programación Científica para Ciencia de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pbWVyntzbvL"
   },
   "source": [
    "### **Cuerpo Docente:**\n",
    "\n",
    "- Profesores: Ignacio Meza, Sebastián Tinoco\n",
    "- Auxiliar: Eduardo Moya\n",
    "- Ayudantes: Nicolás Ojeda, Melanie Peña, Valentina Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy6ikgVYzghB"
   },
   "source": [
    "### **Equipo: SUPER IMPORTANTE - notebooks sin nombre no serán revisados**\n",
    "\n",
    "- Nombre de alumno 1: Tomás Ignacio Reyes Oyarzún"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMJ-owchzjFf"
   },
   "source": [
    "### **Link de repositorio de GitHub:** [Repo - TR](https://github.com/TomiReyes/MDS7202-TR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUuwsXrKzmkK"
   },
   "source": [
    "## **Temas a tratar**\n",
    "\n",
    "- Reinforcement Learning\n",
    "- Large Language Models\n",
    "\n",
    "## **Reglas:**\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n",
    "- Prohibidas las copias.\n",
    "- Pueden usar cualquer matrial del curso que estimen conveniente.\n",
    "\n",
    "### **Objetivos principales del laboratorio**\n",
    "\n",
    "- Resolución de problemas secuenciales usando Reinforcement Learning\n",
    "- Habilitar un Chatbot para entregar respuestas útiles usando Large Language Models.\n",
    "\n",
    "El laboratorio deberá ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al máximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante más eficientes que los iteradores nativos sobre DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hmHHQ9BuyAG"
   },
   "source": [
    "## **1. Reinforcement Learning (2.0 puntos)**\n",
    "\n",
    "En esta sección van a usar métodos de RL para resolver dos problemas interesantes: `Blackjack` y `LunarLander`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOcejYb6uzOO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: gymnasium[box2d]\n"
     ]
    }
   ],
   "source": [
    "#!pip install -qqq gymnasium stable_baselines3\n",
    "#!pip install -qqq swig\n",
    "#!pip install -qqq gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBPet_Mq8dX9"
   },
   "source": [
    "### **1.1 Blackjack (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.recreoviral.com/wp-content/uploads/2016/08/s3.amazonaws.com-Math.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "La idea de esta subsección es que puedan implementar métodos de RL y así generar una estrategia para jugar el clásico juego Blackjack y de paso puedan ~~hacerse millonarios~~ aprender a resolver problemas mediante RL.\n",
    "\n",
    "Comencemos primero preparando el ambiente. El siguiente bloque de código transforma las observaciones del ambiente a `np.array`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LpZ8bBKk9ZlU"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import MultiDiscrete\n",
    "import numpy as np\n",
    "\n",
    "class FlattenObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(FlattenObservation, self).__init__(env)\n",
    "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.array(observation).flatten()\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "env = FlattenObservation(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ6J1_-Y9nHO"
   },
   "source": [
    "#### **1.1.1 Descripción de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripción sobre el ambiente [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/) y su formulación en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5i1Wt1p770x"
   },
   "source": [
    "El ambiante Blackjack representa el juego donde se compite contra el dealer respecto a quien está más cerca del número 21 sumando sus cartas. Dentro del juego se pueden distinguir los siguientes estados: \n",
    "- Suma del número de las cartas actual del jugador.\n",
    "- Carta del dealer.\n",
    "- Si el jugador tiene un As usable, es decir, que valga 11 si no le hace pasar de 21, de lo contrario vale 1. \n",
    "\n",
    "En las acciones se ve lo siguiente: \n",
    "- Pedir cartas, para acercarse lo máximo a 21, si el jugador pasa de ese número pierde. \n",
    "- Detenerse, no pedir más cartas y ver cuanto suma el dealer para comparar. \n",
    "\n",
    "Y en recompesas está: \n",
    "- Si el jugador gana es +1. \n",
    "- Si el juegador pierde -1.\n",
    "- Si empata con el dealer +/- 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmcX6bRC9agQ"
   },
   "source": [
    "#### **1.1.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulación 5000 veces y reporte el promedio y desviación de las recompensas. ¿Cómo calificaría el performance de esta política? ¿Cómo podría interpretar las recompensas obtenidas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se que no se puede usar ciclor for o while pero al hacer algo 5000 veces la verdad no se me ocurrió como hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9p2PrLLR9yju"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensa: -0.4006\n",
      "Desviación estándar de la recompensa: 0.8962810050425033\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 5000\n",
    "rewards = []\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    observation = env.reset()[0]\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  \n",
    "        observation, reward, done, truncated, info = env.step(action)\n",
    "        episode_reward += reward  \n",
    "\n",
    "    rewards.append(episode_reward)  \n",
    "\n",
    "average_reward = np.mean(rewards)\n",
    "std_dev_reward = np.std(rewards)\n",
    "\n",
    "print(f\"Promedio de recompensa: {average_reward}\")\n",
    "print(f\"Desviación estándar de la recompensa: {std_dev_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El promedio de las recompensas siempre se aproxima a -0.4, con una desviación estándar de aprox 0.9. Esto quiere decir que es más normal perder que ganar, lo cual hace sentido por como está hecho el juego y el propósito que tiene. El hecho de tener una desviación estándar alta (respecto al promedio) indica que sí hay casos en que se puede ganar y tener buenos premios, también da a entender que hay alta variabilidad por el factor suerte que hay más allá de la estrategia. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEO_dY4x_SJu"
   },
   "source": [
    "#### **1.1.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `Blackjack`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando PPO, por su comportamiento con los problemas discretos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "m9JsFA1wGmnH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.37     |\n",
      "|    ep_rew_mean     | -0.27    |\n",
      "| time/              |          |\n",
      "|    fps             | 4957     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.28        |\n",
      "|    ep_rew_mean          | -0.46       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3362        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018564414 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.676      |\n",
      "|    explained_variance   | -0.0314     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.256       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0546     |\n",
      "|    value_loss           | 0.749       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.39        |\n",
      "|    ep_rew_mean          | -0.16       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3111        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021009888 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.629      |\n",
      "|    explained_variance   | 0.0945      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.224       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0546     |\n",
      "|    value_loss           | 0.719       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.24        |\n",
      "|    ep_rew_mean          | -0.14       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2995        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027563399 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.562      |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.299       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0388     |\n",
      "|    value_loss           | 0.709       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.29       |\n",
      "|    ep_rew_mean          | -0.08      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 2932       |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01667795 |\n",
      "|    clip_fraction        | 0.11       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.496     |\n",
      "|    explained_variance   | 0.175      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.307      |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0201    |\n",
      "|    value_loss           | 0.722      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x156e69d90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-bpdb8wZID1"
   },
   "source": [
    "#### **1.1.4 Evaluación de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.1.2 pero utilizando el modelo entrenado. ¿Cómo es el performance de su agente? ¿Es mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "5-d7d8GFf7F6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio: -0.1152\n",
      "Desviación estándar de la recompensa: 0.9605878200352116\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=5000)\n",
    "\n",
    "print(f\"Recompensa promedio: {mean_reward}\")\n",
    "print(f\"Desviación estándar de la recompensa: {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa una disminución considerable en el promedio de la recompensa, pasando de un aproximado -0.4 a un -0.1, donde sigue siendo más probable perder en el largo plazo, pero por el alto valor del desviación estándar sigue existiendo alta variabilidad, dandole sentido al factor suerte y las probabilidades de obtener recompensas en el juego. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO-EsAaPAYEm"
   },
   "source": [
    "#### **1.1.5 Estudio de acciones (0.2 puntos)**\n",
    "\n",
    "Genere una función que reciba un estado y retorne la accion del agente. Luego, use esta función para entregar la acción escogida frente a los siguientes escenarios:\n",
    "\n",
    "- Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene tiene un as\n",
    "- Suma de cartas del agente es 19, dealer muestra un 3, agente tiene tiene un as\n",
    "\n",
    "¿Son coherentes sus acciones con las reglas del juego?\n",
    "\n",
    "Hint: ¿A que clase de python pertenecen los estados? Pruebe a usar el método `.reset` para saberlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fh8XlGyzwtRp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase del estado: <class 'numpy.ndarray'>, Estado de ejemplo: [19 10  0]\n"
     ]
    }
   ],
   "source": [
    "def obtener_accion(model, estado):\n",
    "    accion, _ = model.predict([estado], deterministic=True)\n",
    "    return accion[0]  #\n",
    "\n",
    "estado = env.reset()[0]\n",
    "print(f\"Clase del estado: {type(estado)}, Estado de ejemplo: {estado}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acción para el escenario 1: 1\n",
      "Acción para el escenario 2: 0\n"
     ]
    }
   ],
   "source": [
    "estado1 = (6, 7, False)\n",
    "accion1 = obtener_accion(model, estado1)\n",
    "print(f\"Acción para el escenario 1: {accion1}\")\n",
    "\n",
    "estado2 = (19, 3, True)\n",
    "accion2 = obtener_accion(model, estado2)\n",
    "print(f\"Acción para el escenario 2: {accion2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son coherentes las desiciones que se tomaron dado que pidió carta cuando tenía un 6, que es una mano baja y con altas probabilidades de perder, y no pidió carta cuando tenía un 19 que es una buena mano y es muy probable que pierde si pide otra carta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEqCTqqroh03"
   },
   "source": [
    "### **1.2 LunarLander**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.redd.it/097t6tk29zf51.jpg\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la sección 2.1, en esta sección usted se encargará de implementar una gente de RL que pueda resolver el ambiente `LunarLander`.\n",
    "\n",
    "Comencemos preparando el ambiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "nvQUyuZ_FtZ4"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode = \"rgb_array\", continuous = True) # notar el parámetro continuous = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBU4lGX3wpN6"
   },
   "source": [
    "Noten que se especifica el parámetro `continuous = True`. ¿Que implicancias tiene esto sobre el ambiente?\n",
    "\n",
    "Además, se le facilita la función `export_gif` para el ejercicio 2.2.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "bRiWpSo9yfr9"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "def export_gif(model, n = 5):\n",
    "  '''\n",
    "  función que exporta a gif el comportamiento del agente en n episodios\n",
    "  '''\n",
    "  images = []\n",
    "  for episode in range(n):\n",
    "    obs = model.env.reset()\n",
    "    img = model.env.render()\n",
    "    done = False\n",
    "    while not done:\n",
    "      images.append(img)\n",
    "      action, _ = model.predict(obs)\n",
    "      obs, reward, done, info = model.env.step(action)\n",
    "      img = model.env.render(mode=\"rgb_array\")\n",
    "\n",
    "  imageio.mimsave(\"agent_performance.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk5VJVppXh3N"
   },
   "source": [
    "#### **1.2.1 Descripción de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripción sobre el ambiente [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) y su formulación en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas. ¿Como se distinguen las acciones de este ambiente en comparación a `Blackjack`?\n",
    "\n",
    "Nota: recuerde que se especificó el parámetro `continuous = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb-u9LUE8O9a"
   },
   "source": [
    "LunarLander es un juego de aterrizar una nave de manera correcta en el espacio que corresponde. Dentro del juego se pueden distinguir los siguientes estados: \n",
    "- Posición de la nave en el eje x y eje y.\n",
    "- Velocidades en x e y.\n",
    "- Ángulo de orientación y velocidad angular. \n",
    "- Indicador si las patas de aterrizaje izquierda y derecha tocaron el suelo. \n",
    "\n",
    "Considerando el modo continuo, las acciones son: \n",
    "- Impulso del motor vertical \n",
    "- Impulso de motores horizontales. \n",
    "\n",
    "Y en recompesas está: \n",
    "- Recompensa por acercarse a la plataforma y disminuir la velocidad.\n",
    "- +10 puntos por cada pata que toque el suelo.\n",
    "- -0.3 por uso del motor principal, -0.03 por uso de motores horizontales.\n",
    "- +100 puntos aterrizaje exitoso.\n",
    "- -100 puntos por chocar. \n",
    "\n",
    "En este ambiente existen más escenarios, estados, acciones y recompensas que en el blackjack, porque el juego de azar es un ambiente discreto mientras que LunarLander trata de un ambiente continuo con más desiciones que tomar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YChodtNQwzG2"
   },
   "source": [
    "#### **1.2.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulación 10 veces y reporte el promedio y desviación de las recompensas. ¿Cómo calificaría el performance de esta política?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bwc3A0GX7a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensa: -236.68379621334884\n",
      "Desviación estándar de la recompensa: 101.19090565970589\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "rewards = []\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    observation = env.reset()[0]\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "        episode_reward += reward  \n",
    "    rewards.append(episode_reward) \n",
    "\n",
    "average_reward = np.mean(rewards)\n",
    "std_dev_reward = np.std(rewards)\n",
    "\n",
    "print(f\"Promedio de recompensa: {average_reward}\")\n",
    "print(f\"Desviación estándar de la recompensa: {std_dev_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se realiza de manera aleatoria, no tiene sentido dado que se penaliza bastante el chocar o tomar desiciones al azar. Esto se explica con el promedio de recompensa que se aproxima a -250, con una desviación estándar cercana a 100, quiere decir que en la mayoría de los casos se pierde, esto porque se requiere tener más sentido en las desiciones al tratar de un juego continuo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQrZVQflX_5f"
   },
   "source": [
    "#### **1.2.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `LunarLander` **usando 10000 timesteps de entrenamiento**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_6Ia9uoF7Hs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 106      |\n",
      "|    ep_rew_mean     | -153     |\n",
      "| time/              |          |\n",
      "|    fps             | 3994     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 108         |\n",
      "|    ep_rew_mean          | -166        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3017        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004465096 |\n",
      "|    clip_fraction        | 0.0334      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.00285    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 488         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00771    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.37e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 110          |\n",
      "|    ep_rew_mean          | -166         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2808         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034254668 |\n",
      "|    clip_fraction        | 0.0169       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | -0.0173      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 503          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0049      |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 1.25e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 112          |\n",
      "|    ep_rew_mean          | -168         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2730         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071663945 |\n",
      "|    clip_fraction        | 0.0333       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.86        |\n",
      "|    explained_variance   | -0.0466      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 646          |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00498     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 1.2e+03      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 111          |\n",
      "|    ep_rew_mean          | -174         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2681         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044424823 |\n",
      "|    clip_fraction        | 0.0165       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.86        |\n",
      "|    explained_variance   | -0.00352     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 278          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00584     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 812          |\n",
      "------------------------------------------\n",
      "Recompensa promedio: -172.95964538636616\n",
      "Desviación estándar de la recompensa: 121.28234769768235\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3z-oIUSrlAsY"
   },
   "source": [
    "#### **1.2.4 Evaluación de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.2.2 pero utilizando el modelo entrenado. ¿Cómo es el performance de su agente? ¿Es mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ophyU3KrWrwl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio: -155.72273620029446\n",
      "Desviación estándar de la recompensa: 96.92851277984472\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Recompensa promedio: {mean_reward}\")\n",
    "print(f\"Desviación estándar de la recompensa: {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es mejor que el escenario baseline pero sigue siendo negativo, al tratar de un ejercicio continuo puede que el modelo necesite más tiempo para aprender de mejor manera. De todos modos, aumentó la recompensa de -250 a -150 aprox. Con la misma desviación estándar, por lo que el juego aún tiene variabilidad que afecta en su desempeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6Xw4YHT3P5d"
   },
   "source": [
    "#### **1.2.5 Optimización de modelo (0.2 puntos)**\n",
    "\n",
    "Repita los ejercicios 1.2.3 y 1.2.4 hasta obtener un nivel de recompensas promedio mayor a 50. Para esto, puede cambiar manualmente parámetros como:\n",
    "- `total_timesteps`\n",
    "- `learning_rate`\n",
    "- `batch_size`\n",
    "\n",
    "Una vez optimizado el modelo, use la función `export_gif` para estudiar el comportamiento de su agente en la resolución del ambiente y comente sobre sus resultados.\n",
    "\n",
    "Adjunte el gif generado en su entrega (mejor aún si además adjuntan el gif en el markdown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aItYF6sr6F_6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 117      |\n",
      "|    ep_rew_mean     | -281     |\n",
      "| time/              |          |\n",
      "|    fps             | 3953     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 111         |\n",
      "|    ep_rew_mean          | -263        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2581        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007455999 |\n",
      "|    clip_fraction        | 0.0477      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.86       |\n",
      "|    explained_variance   | -0.00574    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.22e+03    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00435    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 1.87e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 109         |\n",
      "|    ep_rew_mean          | -264        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2274        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014346846 |\n",
      "|    clip_fraction        | 0.0628      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | -0.00197    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 447         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0036     |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 1.32e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 112         |\n",
      "|    ep_rew_mean          | -253        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2159        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014127588 |\n",
      "|    clip_fraction        | 0.0587      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.88       |\n",
      "|    explained_variance   | -0.00635    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 380         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00285    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 1.51e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 117         |\n",
      "|    ep_rew_mean          | -266        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2069        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008677239 |\n",
      "|    clip_fraction        | 0.0865      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.89       |\n",
      "|    explained_variance   | -0.00143    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 270         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.000326    |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 693         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 119          |\n",
      "|    ep_rew_mean          | -247         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2011         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071522696 |\n",
      "|    clip_fraction        | 0.0499       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.91        |\n",
      "|    explained_variance   | -0.000664    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 222          |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 1.01e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 122          |\n",
      "|    ep_rew_mean          | -240         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1962         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060688914 |\n",
      "|    clip_fraction        | 0.0879       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.95        |\n",
      "|    explained_variance   | -2.71e-05    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 107          |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00331     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 296          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 123          |\n",
      "|    ep_rew_mean          | -220         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1939         |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059807533 |\n",
      "|    clip_fraction        | 0.0638       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.93        |\n",
      "|    explained_variance   | -1.73e-05    |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 332          |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0032      |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 753          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 126         |\n",
      "|    ep_rew_mean          | -201        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1924        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008148523 |\n",
      "|    clip_fraction        | 0.0863      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.93       |\n",
      "|    explained_variance   | -7.15e-07   |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 198         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00647    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 427         |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 126       |\n",
      "|    ep_rew_mean          | -172      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 1906      |\n",
      "|    iterations           | 10        |\n",
      "|    time_elapsed         | 10        |\n",
      "|    total_timesteps      | 20480     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0057433 |\n",
      "|    clip_fraction        | 0.0813    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.92     |\n",
      "|    explained_variance   | -1.07e-06 |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 132       |\n",
      "|    n_updates            | 90        |\n",
      "|    policy_gradient_loss | -0.00671  |\n",
      "|    std                  | 1.04      |\n",
      "|    value_loss           | 295       |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 125         |\n",
      "|    ep_rew_mean          | -142        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1882        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015755914 |\n",
      "|    clip_fraction        | 0.0808      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.92       |\n",
      "|    explained_variance   | -5.13e-06   |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 211         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00188    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 327         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 126         |\n",
      "|    ep_rew_mean          | -134        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1872        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004406758 |\n",
      "|    clip_fraction        | 0.0731      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.89       |\n",
      "|    explained_variance   | 2.32e-06    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 211         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00399    |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 422         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 134         |\n",
      "|    ep_rew_mean          | -134        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1664        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005114861 |\n",
      "|    clip_fraction        | 0.0242      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.89       |\n",
      "|    explained_variance   | 0.0324      |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 229         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00412    |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 393         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 138         |\n",
      "|    ep_rew_mean          | -120        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1668        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005577782 |\n",
      "|    clip_fraction        | 0.0555      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.89       |\n",
      "|    explained_variance   | 0.172       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 147         |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00926    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 301         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 150          |\n",
      "|    ep_rew_mean          | -115         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1573         |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068000676 |\n",
      "|    clip_fraction        | 0.0422       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.89        |\n",
      "|    explained_variance   | 0.278        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 58.3         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00661     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 249          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 152          |\n",
      "|    ep_rew_mean          | -104         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1583         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047580325 |\n",
      "|    clip_fraction        | 0.0391       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.89        |\n",
      "|    explained_variance   | 0.361        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 39.9         |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00382     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 291          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 157         |\n",
      "|    ep_rew_mean          | -99.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1583        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005592329 |\n",
      "|    clip_fraction        | 0.0458      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.9        |\n",
      "|    explained_variance   | 0.482       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 85.9        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00662    |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 139         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 164          |\n",
      "|    ep_rew_mean          | -96.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1546         |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076021384 |\n",
      "|    clip_fraction        | 0.0695       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.9         |\n",
      "|    explained_variance   | 0.326        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 152          |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0119      |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 350          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 177          |\n",
      "|    ep_rew_mean          | -87.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1500         |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055795424 |\n",
      "|    clip_fraction        | 0.0513       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.91        |\n",
      "|    explained_variance   | 0.736        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 52.4         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00523     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 104          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 182         |\n",
      "|    ep_rew_mean          | -81         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1486        |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008908619 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.9        |\n",
      "|    explained_variance   | 0.748       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 52.4        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 98          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 197         |\n",
      "|    ep_rew_mean          | -78.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1431        |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009169552 |\n",
      "|    clip_fraction        | 0.0806      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.88       |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 22.1        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0068     |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 78.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 217         |\n",
      "|    ep_rew_mean          | -73.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1358        |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007789369 |\n",
      "|    clip_fraction        | 0.0814      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 16.9        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00416    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 84          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 230         |\n",
      "|    ep_rew_mean          | -66.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1317        |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015333308 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.8        |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 7.54        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    std                  | 0.97        |\n",
      "|    value_loss           | 39.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 234         |\n",
      "|    ep_rew_mean          | -56.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1296        |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013030836 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.74       |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 21.9        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    std                  | 0.938       |\n",
      "|    value_loss           | 54.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 247          |\n",
      "|    ep_rew_mean          | -53.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1255         |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 40           |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073054237 |\n",
      "|    clip_fraction        | 0.0895       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 18.6         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00424     |\n",
      "|    std                  | 0.934        |\n",
      "|    value_loss           | 40.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 263         |\n",
      "|    ep_rew_mean          | -49         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1225        |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011356565 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.69       |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 14          |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00659    |\n",
      "|    std                  | 0.931       |\n",
      "|    value_loss           | 58.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 277         |\n",
      "|    ep_rew_mean          | -47.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1193        |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011148443 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.65       |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 12.5        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00892    |\n",
      "|    std                  | 0.901       |\n",
      "|    value_loss           | 37.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 291         |\n",
      "|    ep_rew_mean          | -39.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1152        |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011850733 |\n",
      "|    clip_fraction        | 0.0895      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.6        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.76        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00175    |\n",
      "|    std                  | 0.88        |\n",
      "|    value_loss           | 34.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 308         |\n",
      "|    ep_rew_mean          | -37.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1126        |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008032192 |\n",
      "|    clip_fraction        | 0.0914      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.59       |\n",
      "|    explained_variance   | 0.801       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 6.54        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00388    |\n",
      "|    std                  | 0.886       |\n",
      "|    value_loss           | 107         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 321         |\n",
      "|    ep_rew_mean          | -32.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1105        |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009597252 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.57       |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 12.7        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00391    |\n",
      "|    std                  | 0.873       |\n",
      "|    value_loss           | 103         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 327         |\n",
      "|    ep_rew_mean          | -31.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1100        |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007125445 |\n",
      "|    clip_fraction        | 0.0616      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.53       |\n",
      "|    explained_variance   | 0.773       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 71.8        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00516    |\n",
      "|    std                  | 0.847       |\n",
      "|    value_loss           | 165         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 343         |\n",
      "|    ep_rew_mean          | -30.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1090        |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006693977 |\n",
      "|    clip_fraction        | 0.0882      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.51       |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 66          |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00613    |\n",
      "|    std                  | 0.855       |\n",
      "|    value_loss           | 60.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 352         |\n",
      "|    ep_rew_mean          | -29.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1084        |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008458933 |\n",
      "|    clip_fraction        | 0.0819      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.51       |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 169         |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00319    |\n",
      "|    std                  | 0.845       |\n",
      "|    value_loss           | 113         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 372        |\n",
      "|    ep_rew_mean          | -25.5      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1077       |\n",
      "|    iterations           | 34         |\n",
      "|    time_elapsed         | 64         |\n",
      "|    total_timesteps      | 69632      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00971023 |\n",
      "|    clip_fraction        | 0.0915     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.53      |\n",
      "|    explained_variance   | 0.931      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 55.8       |\n",
      "|    n_updates            | 330        |\n",
      "|    policy_gradient_loss | -0.0066    |\n",
      "|    std                  | 0.868      |\n",
      "|    value_loss           | 61.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 390         |\n",
      "|    ep_rew_mean          | -24         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1056        |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 67          |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005721221 |\n",
      "|    clip_fraction        | 0.0846      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.53       |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 7.56        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.000487   |\n",
      "|    std                  | 0.858       |\n",
      "|    value_loss           | 112         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 403         |\n",
      "|    ep_rew_mean          | -18.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1054        |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 69          |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008174409 |\n",
      "|    clip_fraction        | 0.0898      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.49       |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.6         |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00233    |\n",
      "|    std                  | 0.836       |\n",
      "|    value_loss           | 15.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 414          |\n",
      "|    ep_rew_mean          | -14.8        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1039         |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 72           |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056904564 |\n",
      "|    clip_fraction        | 0.0561       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.47        |\n",
      "|    explained_variance   | 0.844        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 191          |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00391     |\n",
      "|    std                  | 0.833        |\n",
      "|    value_loss           | 119          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 432         |\n",
      "|    ep_rew_mean          | -7.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1037        |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 74          |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008249228 |\n",
      "|    clip_fraction        | 0.0795      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.46       |\n",
      "|    explained_variance   | 0.649       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 114         |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00715    |\n",
      "|    std                  | 0.833       |\n",
      "|    value_loss           | 189         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 448          |\n",
      "|    ep_rew_mean          | -1.81        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1030         |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 77           |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067386613 |\n",
      "|    clip_fraction        | 0.0468       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.45        |\n",
      "|    explained_variance   | 0.736        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 171          |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.00258     |\n",
      "|    std                  | 0.824        |\n",
      "|    value_loss           | 320          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 456         |\n",
      "|    ep_rew_mean          | 3.28        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1025        |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006938911 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.42       |\n",
      "|    explained_variance   | 0.77        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 20.9        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00165    |\n",
      "|    std                  | 0.816       |\n",
      "|    value_loss           | 104         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 460          |\n",
      "|    ep_rew_mean          | 15.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1027         |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 81           |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076231314 |\n",
      "|    clip_fraction        | 0.0949       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.41        |\n",
      "|    explained_variance   | 0.849        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 21.4         |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00464     |\n",
      "|    std                  | 0.808        |\n",
      "|    value_loss           | 95.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 476          |\n",
      "|    ep_rew_mean          | 18.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1018         |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 84           |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0108247455 |\n",
      "|    clip_fraction        | 0.0765       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.38        |\n",
      "|    explained_variance   | 0.666        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 25           |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00743     |\n",
      "|    std                  | 0.79         |\n",
      "|    value_loss           | 190          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 487         |\n",
      "|    ep_rew_mean          | 24.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1011        |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 87          |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011545405 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.34       |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 21.6        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00396    |\n",
      "|    std                  | 0.784       |\n",
      "|    value_loss           | 131         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 503          |\n",
      "|    ep_rew_mean          | 33.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1009         |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 89           |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071927104 |\n",
      "|    clip_fraction        | 0.0811       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.35        |\n",
      "|    explained_variance   | 0.835        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 13.8         |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | 0.000288     |\n",
      "|    std                  | 0.785        |\n",
      "|    value_loss           | 124          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 506          |\n",
      "|    ep_rew_mean          | 40.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1004         |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 91           |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062534343 |\n",
      "|    clip_fraction        | 0.0795       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.33        |\n",
      "|    explained_variance   | 0.828        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 26.3         |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00245     |\n",
      "|    std                  | 0.777        |\n",
      "|    value_loss           | 106          |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 514        |\n",
      "|    ep_rew_mean          | 40.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1000       |\n",
      "|    iterations           | 46         |\n",
      "|    time_elapsed         | 94         |\n",
      "|    total_timesteps      | 94208      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01172702 |\n",
      "|    clip_fraction        | 0.114      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.27      |\n",
      "|    explained_variance   | 0.792      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 29.5       |\n",
      "|    n_updates            | 450        |\n",
      "|    policy_gradient_loss | -0.00501   |\n",
      "|    std                  | 0.747      |\n",
      "|    value_loss           | 120        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 511         |\n",
      "|    ep_rew_mean          | 38          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 998         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 96          |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009133062 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.25       |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 7.14        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.00678    |\n",
      "|    std                  | 0.754       |\n",
      "|    value_loss           | 110         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 515         |\n",
      "|    ep_rew_mean          | 45.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 995         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 98          |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007131334 |\n",
      "|    clip_fraction        | 0.0674      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.26       |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 42.1        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0033     |\n",
      "|    std                  | 0.756       |\n",
      "|    value_loss           | 191         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 528         |\n",
      "|    ep_rew_mean          | 53.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 989         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 101         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010556026 |\n",
      "|    clip_fraction        | 0.0981      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.24       |\n",
      "|    explained_variance   | 0.773       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 65.6        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.000774   |\n",
      "|    std                  | 0.741       |\n",
      "|    value_loss           | 157         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa promedio: 120.71579263039924\n",
      "Desviación estándar de la recompensa: 82.10914774619216\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-4  \n",
    "batch_size = 32       \n",
    "total_timesteps = 100000 \n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, learning_rate=learning_rate, batch_size=batch_size)\n",
    "\n",
    "model.learn(total_timesteps=total_timesteps)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Recompensa promedio: {mean_reward}\")\n",
    "print(f\"Desviación estándar de la recompensa: {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_gif(model, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Agent Performance GIF](agent_performance.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPUY-Ktgf2BO"
   },
   "source": [
    "## **2. Large Language Models (4.0 puntos)**\n",
    "\n",
    "En esta sección se enfocarán en habilitar un Chatbot que nos permita responder preguntas útiles a través de LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ4fPRRihGLe"
   },
   "source": [
    "### **2.0 Configuración Inicial**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/uqAs9atZH58AAAAd/config-config-issue.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Como siempre, cargamos todas nuestras API KEY al entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ud2Xm_k-hFJn"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj9JvQUsgZZJ"
   },
   "source": [
    "### **2.1 Retrieval Augmented Generation (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://y.yarn.co/218aaa02-c47e-4ec9-b1c9-07792a06a88f_text.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsección es que habiliten un chatbot que pueda responder preguntas usando información contenida en documentos PDF a través de **Retrieval Augmented Generation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrxOQroVnaZ5"
   },
   "source": [
    "#### **2.1.1 Reunir Documentos (0 puntos)**\n",
    "\n",
    "Reuna documentos PDF sobre los que hacer preguntas siguiendo las siguientes instrucciones:\n",
    "  - 2 documentos .pdf como mínimo.\n",
    "  - 50 páginas de contenido como mínimo entre todos los documentos.\n",
    "  - Ideas para documentos: Documentos relacionados a temas académicos, laborales o de ocio. Aprovechen este ejercicio para construir algo útil y/o relevante para ustedes!\n",
    "  - Deben ocupar documentos reales, no pueden utilizar los mismos de la clase.\n",
    "  - Deben registrar sus documentos en la siguiente [planilla](https://docs.google.com/spreadsheets/d/1Hy1w_dOiG2UCHJ8muyxhdKPZEPrrL7BNHm6E90imIIM/edit?usp=sharing). **NO PUEDEN USAR LOS MISMOS DOCUMENTOS QUE OTRO GRUPO**\n",
    "  - **Recuerden adjuntar los documentos en su entrega**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5D1tIRCi4oJJ",
    "outputId": "39f6d4fc-63cb-4b9b-d48f-48d60df25ee5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install --upgrade --quiet PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kzq2TjWCnu15"
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "doc_paths = ['/Users/tomasreyes/Desktop/Cosas U /Laboratorio de Programación Científica/MDS7202-1/pdfs/analysing-soccer-cluster-concepts.pdf', \n",
    "             \"/Users/tomasreyes/Desktop/Cosas U /Laboratorio de Programación Científica/MDS7202-1/pdfs/Clustering_soccer_players_investigating.pdf\", \n",
    "             \"/Users/tomasreyes/Desktop/Cosas U /Laboratorio de Programación Científica/MDS7202-1/pdfs/ml-approach-xg.pdf\"] # rellenar con los path a sus documentos\n",
    "\n",
    "assert len(doc_paths) >= 2, \"Deben adjuntar un mínimo de 2 documentos\"\n",
    "\n",
    "total_paginas = sum(len(PyPDF2.PdfReader(open(doc, \"rb\")).pages) for doc in doc_paths)\n",
    "assert total_paginas >= 50, f\"Páginas insuficientes: {total_paginas}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r811-P71nizA"
   },
   "source": [
    "#### **2.1.2 Vectorizar Documentos (0.2 puntos)**\n",
    "\n",
    "Vectorice los documentos y almacene sus representaciones de manera acorde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "n-yXAdCSn4JM"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader1 = PyPDFLoader(doc_paths[0])\n",
    "loader2 = PyPDFLoader(doc_paths[1])\n",
    "loader3 = PyPDFLoader(doc_paths[2])\n",
    "\n",
    "docs = loader1.load() + loader2.load() + loader3.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50) \n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x12e67a410>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\") \n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embedding) \n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAUkP5zrnyBK"
   },
   "source": [
    "#### **2.1.3 Habilitar RAG (0.3 puntos)**\n",
    "\n",
    "Habilite la solución RAG a través de una *chain* y guárdela en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", \n",
    "                                     search_kwargs={\"k\": 3}, # n° documentos a recuperar\n",
    "                                     )\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"Usando el contexto siguiente:\\n{context}\\nResponde la pregunta: {question}\"\n",
    ")\n",
    "\n",
    "qa_chain = load_qa_chain(llm=llm, chain_type=\"stuff\", prompt=prompt_template)\n",
    "\n",
    "rag_chain = RetrievalQA(\n",
    "    retriever=retriever,\n",
    "    combine_documents_chain=qa_chain\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycg5S5i_n-kL"
   },
   "source": [
    "#### **2.1.4 Verificación de respuestas (0.5 puntos)**\n",
    "\n",
    "Genere un listado de 3 tuplas (\"pregunta\", \"respuesta correcta\") y analice la respuesta de su solución para cada una. ¿Su solución RAG entrega las respuestas que esperaba?\n",
    "\n",
    "Ejemplo de tupla:\n",
    "- Pregunta: ¿Quién es el presidente de Chile?\n",
    "- Respuesta correcta: El presidente de Chile es Gabriel Boric\n",
    "\n",
    "Caso1: \n",
    "- Pregunta: ¿Cuál es el objetivo principal del documento sobre Expected Goals (xG)?\n",
    "- Respuesta correcta: Desarrollar un modelo de Expected Goals (xG) ajustado para cada posición y jugador en fútbol, mejorando la precisión en la predicción de goles según datos de StatsBomb.\n",
    "\n",
    "Caso2: \n",
    "- Pregunta: ¿Qué métodos de clustering se utilizan en el documento de clustering de jugadores de fútbol?\n",
    "- Respuesta correcta: El documento utiliza K-means y Expectation Maximization (EM) para agrupar a jugadores según su posición, evaluando la pureza y el puntaje de silhouette.\n",
    "\n",
    "Caso3: \n",
    "- Pregunta: ¿Qué es un \"conceptor\" en el contexto del análisis de juegos de fútbol?\n",
    "- Respuesta correcta: Un \"conceptor\" es una herramienta de redes neuronales recurrentes que ayuda a identificar y predecir patrones repetitivos en los juegos de fútbol simulados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "S_UiEn1hoZYR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '¿Cuál es el objetivo principal del documento sobre Expected Goals (xG)?', 'result': 'El objetivo principal del documento es crear y aplicar un modelo de Expected Goals (xG) \"desde cero\" para evaluar el rendimiento de equipos e individuos.\\n'}\n"
     ]
    }
   ],
   "source": [
    "respuesta1 = rag_chain.invoke({\"query\": \"¿Cuál es el objetivo principal del documento sobre Expected Goals (xG)?\"})\n",
    "print(respuesta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '¿Qué métodos de clustering se utilizan en el documento de clustering de jugadores de fútbol?', 'result': 'El documento utiliza dos métodos de clustering: K-means y Expectation Maximization (EM).\\n'}\n"
     ]
    }
   ],
   "source": [
    "respuesta2 = rag_chain.invoke({\"query\": \"¿Qué métodos de clustering se utilizan en el documento de clustering de jugadores de fútbol?\"})\n",
    "print(respuesta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En el contexto del análisis de juegos de fútbol, un \"conceptor\" es una matriz de baja dimensionalidad (una variedad de baja dimensión) que describe trayectorias a través de un espacio de estados de alta dimensionalidad.  Estos permiten predicciones específicas de la situación a partir de la misma red neuronal recurrente.  En esencia, los conceptores restringen la dinámica de la red neuronal a un subespacio lineal característico de un patrón específico (como un movimiento en el juego), permitiendo identificar y analizar movimientos como \"pase\" o \"regate\" sin necesidad de definirlos previamente.  Permiten segmentar el juego en secuencias de situaciones aprendidas de forma no supervisada y predecir el futuro cercano de cada situación.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "respuesta3 = rag_chain.run({\"query\": \"¿Qué es un 'conceptor' en el contexto del análisis de juegos de fútbol?\"})\n",
    "print(respuesta3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8d5zTMHoUgF"
   },
   "source": [
    "#### **2.1.5 Sensibilidad de Hiperparámetros (0.5 puntos)**\n",
    "\n",
    "Extienda el análisis del punto 2.1.4 analizando cómo cambian las respuestas entregadas cambiando los siguientes hiperparámetros:\n",
    "- `Tamaño del chunk`. (*¿Cómo repercute que los chunks sean mas grandes o chicos?*)\n",
    "- `La cantidad de chunks recuperados`. (*¿Qué pasa si se devuelven muchos/pocos chunks?*)\n",
    "- `El tipo de búsqueda`. (*¿Cómo afecta el tipo de búsqueda a las respuestas de mi RAG?*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tamaño del chunk: El tamaño del chunk afecta en la vectorización de los documentos por lo que si el tamaño del chunk es menor, afecta en la eficiencia del modelo. Ahora pasando a las respuestas: con chunks más grandes (1000 en este caso) la respuesta es más larga aunque las dos tienen información similar, la respuesta con chunk de 500 es más precisa, por lo tanto, es mejor. Comparando con chunks de menor tamaño (100 en este caso) la respuesta es más corta y pierde harta información, por lo que no es correcta o está muy incompleta.\n",
    "\n",
    "    - Chunk de 1000: 'El objetivo principal del documento es crear y aplicar un modelo de Expected Goals (xG) \"desde cero\" para predecir xG,  desarrollar métricas xG ajustadas por posición y jugador, y proporcionar ventajas competitivas a la industria y mejoras en el conocimiento académico.  Esto incluye la creación de un modelo confiable y robusto que pueda predecir goles con precisión, evaluar la calidad del remate de los jugadores y lograr una correlación positiva significativa con los proveedores de xG de la industria.\\n'\n",
    "\n",
    "    - Chunk de 500 (utilizado): 'El objetivo principal del documento es crear y aplicar un modelo de Expected Goals (xG) \"desde cero\", utilizando características nuevas y altamente informativas para predecir los valores de xG.  También busca proporcionar ventajas competitivas a la industria y mejorar el conocimiento académico a través de la publicación de un nuevo modelo de xG ajustado por posición y jugador.\\n'\n",
    "\n",
    "    - Chunk de 100: El objetivo principal del documento es crear y aplicar un modelo de Expected Goals (xG) \"desde cero\" para evaluar el rendimiento de equipos e individuos\n",
    "\n",
    "\n",
    "* Cantidad de chunks recuperados: La cantidad de chunks recuperados afecta en la complejidad de la respuesta dado que define cuantos fragmentos se van a utilizar para realizar la respuesta. Con más chunks la respuesta debería ser más completa pero con riesgo de tener información irrelevante, con menos chunks puede que no se logre obtener la información esperada\n",
    "\n",
    "    - 1 recuperado: 'El texto no especifica los métodos de clustering utilizados.  Solo menciona que se agrupan los jugadores en 4 y 11 clusters basándose en su posición (portero, defensa, mediocampista, atacante) y la frecuencia de eventos que realizan.  No se nombra ningún algoritmo de clustering específico (como k-means, jerárquico, etc.).\\n'\n",
    "\n",
    "    - 3 recuperados: 'El documento utiliza dos métodos de clustering: K-means y Expectation Maximization (EM)'\n",
    "\n",
    "    - 5 recuperados: 'El documento utiliza dos métodos de clustering no supervisados: K-means y Expectation Maximization (EM).\\n'\n",
    "\n",
    "\n",
    "* Tipo de búsqueda: El tipo de búsqueda define como se van a seleccionar los chunks para buscar la respuesta, aquí las respuestas con 3 distintos tipos de búsqueda:\n",
    "\n",
    "    - Similarity: 'En el contexto del análisis de juegos de fútbol, un \"conceptor\" es una matriz de baja dimensionalidad (una variedad de baja dimensión) que describe trayectorias a través de un espacio de estados de alta dimensionalidad.  Estos permiten predicciones específicas de la situación a partir de la misma red neuronal recurrente.  En esencia, los conceptores restringen la dinámica de la red neuronal a un subespacio lineal característico de un patrón específico (como un movimiento en el juego), permitiendo identificar y analizar movimientos como \"pase\" o \"regate\" sin necesidad de definirlos previamente.  Permiten segmentar el juego en secuencias de situaciones aprendidas de forma no supervisada y predecir el futuro cercano de cada situación.'\n",
    "\n",
    "    - Maximal marginal relevance: 'El texto proporcionado no define explícitamente qué es un \"conceptor\" en el contexto del análisis de juegos de fútbol.  El título menciona \"Clustering and Conceptors\", sugiriendo que son una herramienta o técnica utilizada junto con el clustering (agrupamiento) para analizar partidos de fútbol, pero no se ofrece una explicación de su función o significado.'\n",
    "\n",
    "    - Similarity Score Threshold (score_threshold = 0.5): En el contexto del análisis de juegos de fútbol, un \"conceptor\" se asocia con diferentes movimientos dentro del juego.  El texto no define explícitamente qué *es* un concepter, pero implica que son unidades o categorías que representan acciones o secuencias de acciones dentro de un partido.  El objetivo del estudio es identificar estos \"conceptos\" a partir de datos, utilizando métodos de clustering para agrupar movimientos y establecer un repertorio de los mismos que conforman el juego completo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENJiPPM0giX8"
   },
   "source": [
    "### **2.2 Agentes (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/rcqnN2aJCSEAAAAd/secret-agent-man.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la sección anterior, en esta sección se busca habilitar **Agentes** para obtener información a través de tools y así responder la pregunta del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V47l7Mjfrk0N"
   },
   "source": [
    "#### **2.2.1 Tool de Tavily (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas al motor de búsqueda **Tavily**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6SLKwcWr0AG"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults(max_results = 1) \n",
    "tools = [search] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SonB1A-9rtRq"
   },
   "source": [
    "#### **2.2.2 Tool de Wikipedia (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas a **Wikipedia**.\n",
    "\n",
    "*Hint: Le puede ser de ayuda el siguiente [link](https://python.langchain.com/v0.1/docs/modules/tools/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ehJJpoqsr26-"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=100)\n",
    "tool = WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvUIMdX6r0ne"
   },
   "source": [
    "#### **2.2.3 Crear Agente (0.3 puntos)**\n",
    "\n",
    "Crear un agente que pueda responder preguntas preguntas usando las *tools* antes generadas. Asegúrese que su agente responda en español. Por último, guarde el agente en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "react_prompt = hub.pull(\"hwchase17/react\") \n",
    "print(react_prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, agent=RunnableAgent(runnable=RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_log_to_str(x['intermediate_steps']))\n",
       "})\n",
       "| PromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={}, partial_variables={'tools': \"TavilySearch(tool_input: 'Union[str, dict[str, Any]]', verbose: 'Optional[bool]' = None, start_color: 'Optional[str]' = 'green', color: 'Optional[str]' = 'green', callbacks: 'Callbacks' = None, *, tags: 'Optional[list[str]]' = None, metadata: 'Optional[dict[str, Any]]' = None, run_name: 'Optional[str]' = None, run_id: 'Optional[uuid.UUID]' = None, config: 'Optional[RunnableConfig]' = None, tool_call_id: 'Optional[str]' = None, **kwargs: 'Any') -> 'Any' - Busca resultados en Tavily.\\nWikipediaQuery(tool_input: 'Union[str, dict[str, Any]]', verbose: 'Optional[bool]' = None, start_color: 'Optional[str]' = 'green', color: 'Optional[str]' = 'green', callbacks: 'Callbacks' = None, *, tags: 'Optional[list[str]]' = None, metadata: 'Optional[dict[str, Any]]' = None, run_name: 'Optional[str]' = None, run_id: 'Optional[uuid.UUID]' = None, config: 'Optional[RunnableConfig]' = None, tool_call_id: 'Optional[str]' = None, **kwargs: 'Any') -> 'Any' - Consulta Wikipedia.\", 'tool_names': 'TavilySearch, WikipediaQuery'}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'react', 'lc_hub_commit_hash': 'd15fe3c426f1c4b3f37c9198853e4a86e20c425ca7f4752ec0c9b0e97ca7ea4d'}, template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}')\n",
       "| RunnableBinding(bound=ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), temperature=0.0, max_retries=2, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x1452bd8d0>, default_metadata=()), kwargs={'stop': ['\\nObservation']}, config={}, config_factories=[])\n",
       "| ReActSingleInputOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[Tool(name='TavilySearch', description='Busca resultados en Tavily.', func=<bound method BaseTool.run of TavilySearchResults(max_results=1, api_wrapper=TavilySearchAPIWrapper(tavily_api_key=SecretStr('**********')))>), Tool(name='WikipediaQuery', description='Consulta Wikipedia.', func=<bound method BaseTool.run of WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/opt/homebrew/lib/python3.11/site-packages/wikipedia/__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=100))>)])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain.agents import Tool\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"TavilySearch\", func=search.run, description=\"Busca resultados en Tavily.\"),\n",
    "    Tool(name=\"WikipediaQuery\", func=tool.run, description=\"Consulta Wikipedia.\")\n",
    "]\n",
    "\n",
    "agent = create_react_agent(llm, tools, react_prompt) \n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) \n",
    "agent_executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKV0JxK3r-XG"
   },
   "source": [
    "#### **2.2.4 Verificación de respuestas (0.3 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente y asegúrese que el agente esté ocupando correctamente las tools disponibles. ¿En qué casos el agente debería ocupar la tool de Tavily? ¿En qué casos debería ocupar la tool de Wikipedia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Pqo2dsxvywW_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To answer this question, I need to find information about the winner of the 2022 FIFA World Cup.  Wikipedia is a reliable source for this type of information.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"2022 FIFA World Cup winner\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: qué equipo ganó el mundial de fútbol 2022?\n",
      "Thought:To answer this question, I need to find information about the winner of the 2022 FIFA World Cup.  Wikipedia is a reliable source for this type of information.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"2022 FIFA World Cup winner\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: qué equipo ganó el mundial de fútbol 2022?\n",
      "Thought:To answer this question, I need to find information about the winner of the 2022 FIFA World Cup.  Wikipedia is a reliable source for this type of information.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"2022 FIFA World Cup winner\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: qué equipo ganó el mundial de fútbol 2022?\n",
      "Thought:To answer this question, I need to find information about the winner of the 2022 FIFA World Cup.  Wikipedia is a reliable source for this type of information.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"2022 FIFA World Cup winner\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: qué equipo ganó el mundial de fútbol 2022?\n",
      "Thought:To answer this question, I need to find information about the winner of the 2022 FIFA World Cup.  Wikipedia is a reliable source for this type of information.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"2022 FIFA World Cup winner\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mThought:To answer this question, I need to find information about the winner of the 2022 FIFA World Cup.  Wikipedia is a reliable source for this type of information.  The previous attempts only showed the summary, I need to extract the winner from the page content.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"2022 FIFA World Cup winner\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: qué equipo ganó el mundial de fútbol 2022?\n",
      "Thought:To answer this question, I need to find information about the winner of the 2022 FIFA World Cup.  Wikipedia is a reliable source for this type of information.  I need to ensure I get the full page content, not just the summary, to find the winner.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"2022 FIFA World Cup\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mThought: The previous attempts only returned the summary. I need to find a way to access the full Wikipedia page content to extract the winner.  I'll try a different approach.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: {\"query\": \"2022 FIFA World Cup\", \"extract\": True}\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to use a more specific query to get the winner of the 2022 FIFA World Cup from Wikipedia.  The previous attempts failed to extract the relevant information.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: {\"query\": \"2022 FIFA World Cup\", \"extract\": True}\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to find a way to get the full text of the Wikipedia article for the 2022 FIFA World Cup.  The previous attempts using the `extract` parameter didn't work.  I'll try a different approach, focusing on getting the full page content and then processing it to find the winner.  I'll assume that the winner is explicitly mentioned in the article.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"2022 FIFA World Cup\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to find a way to get the full text of the Wikipedia article for the 2022 FIFA World Cup.  The previous attempts using the `extract` parameter didn't work, and only returned summaries.  I'll try a different approach, focusing on getting the full page content and then processing it to find the winner.  I'll assume that the winner is explicitly mentioned in the article.  I will need to use a different tool or method to extract the winner from the full text.  Since I don't have text processing capabilities, I will rely on the Wikipedia page being well-structured enough that a simple search for \"winner\" will suffice.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"2022 FIFA World Cup\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mThought: The previous attempts to extract the winner from the Wikipedia page have failed.  The provided tools don't seem to offer robust text processing capabilities to extract the answer from the full page content.  I need a different approach.  I will try a simpler query directly asking for the winner.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Who won the 2022 FIFA World Cup?\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mThought: The previous attempts have failed to extract the winner from the Wikipedia page.  The tools provided lack the ability to process the full page text effectively.  I will try a different search engine that might provide a more concise answer.  However, since I only have access to WikipediaQuery and TavilySearch, and neither are designed for this type of text extraction, I will attempt one last Wikipedia query with a more direct question.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Argentina 2022 FIFA World Cup\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup final\n",
      "Summary: The 2022 FIFA World Cup final was the final match of the 20\u001b[0m\u001b[32;1m\u001b[1;3mThought: I will try a final Wikipedia query, focusing on the final match to find the winner.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"2022 FIFA World Cup Final Winner\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup final\n",
      "Summary: The 2022 FIFA World Cup final was the final match of the 20\u001b[0m\u001b[32;1m\u001b[1;3mThought: I have tried various queries on Wikipedia, but haven't been able to extract the winner reliably. The tools provided lack robust text processing capabilities.  I will need to assume that a simple search for \"winner\" within the Wikipedia article would have worked if the tools allowed for that.  Based on my general knowledge, I know Argentina won.\n",
      "\n",
      "Final Answer: Argentina\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Argentina\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"qué equipo ganó el mundial de fútbol 2022?\"})\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Para responder a la pregunta \"¿Cuál es el mejor restaurante en Santiago?\", necesito buscar información sobre restaurantes en Santiago y sus reseñas.  No tengo acceso a información en tiempo real, incluyendo reseñas de restaurantes, por lo que usar TavilySearch podría ser útil si tiene información sobre rankings o reseñas de restaurantes.  WikipediaQuery probablemente no sea tan útil para esta pregunta específica, ya que Wikipedia no suele tener rankings subjetivos de restaurantes.\n",
      "\n",
      "Action: TavilySearch\n",
      "Action Input: \"mejores restaurantes Santiago Chile\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://santiagosecreto.com/mejores-restaurantes-santiago/', 'content': 'Es por eso que lidera la lista de los mejores restaurantes de Santiago. Nueva de Lyon 99, Providencia, Santiago de Chile. Lunes a sábado 13:00 a 23:00 hrs. 2. La Vinoteca. Santiago Secreto. La Vinoteca es tienda, restaurante y una charcutería que te entrega una experiencia completa a través de la historia del vino.'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: La observación de TavilySearch proporciona un enlace a una lista de los mejores restaurantes de Santiago según santiagosecreto.com.  Para obtener una respuesta más completa, debería acceder al enlace y extraer la información relevante. Sin embargo, como no puedo acceder a internet, solo puedo usar la información proporcionada en la observación.\n",
      "\n",
      "Thought: I now know the final answer.  Based on the limited information, one of the best restaurants in Santiago, according to santiagosecreto.com, is mentioned as \"Es por eso que lidera la lista de los mejores restaurantes de Santiago\".  The specific name isn't provided in the snippet, but the website is cited as the source.\n",
      "\n",
      "Final Answer: Según el sitio web santiagosecreto.com, uno de los mejores restaurantes de Santiago es mencionado como el que \"lidera la lista de los mejores restaurantes de Santiago\", aunque el nombre específico no se proporciona en el fragmento de texto.  Para una lista más completa, se debe consultar el enlace proporcionado: https://santiagosecreto.com/mejores-restaurantes-santiago/\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Según el sitio web santiagosecreto.com, uno de los mejores restaurantes de Santiago es mencionado como el que \"lidera la lista de los mejores restaurantes de Santiago\", aunque el nombre específico no se proporciona en el fragmento de texto.  Para una lista más completa, se debe consultar el enlace proporcionado: https://santiagosecreto.com/mejores-restaurantes-santiago/\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"¿Cuál es el mejor restaurante en Santiago?\"})\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se ve en las respuestas, usa Wikipedia para respuestas que tienen más contexto o que pueden ser respondidas a partir de información documentada, en cambio usa Tavily cuando la respuesta no es tan directa o cuando hay menos información objetiva del tema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZbDTYiogquv"
   },
   "source": [
    "### **2.3 Multi Agente (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/r7QMJLxU4BoAAAAd/this-is-getting-out-of-hand-star-wars.gif\"\n",
    "\" width=\"450\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsección es encapsular las funcionalidades creadas en una solución multiagente con un **supervisor**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-iUfH0WvI6m"
   },
   "source": [
    "#### **2.3.1 Generando Tools (0.5 puntos)**\n",
    "\n",
    "Transforme la solución RAG de la sección 2.1 y el agente de la sección 2.2 a *tools* (una tool por cada uno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_tool_function(query: str) -> str:\n",
    "    try:\n",
    "        respuesta = agent_executor.invoke({\"input\": query})\n",
    "        return respuesta[\"output\"]  \n",
    "    except Exception as e:\n",
    "        return f\"Error en AgentTool: {e}\"\n",
    "\n",
    "def rag_tool_function(query: str) -> str:\n",
    "    try:\n",
    "        respuesta = rag_chain.run({\"query\": query}) \n",
    "        return respuesta\n",
    "    except Exception as e:\n",
    "        return f\"Error en RAGTool: {e}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQYNjT_0vPCg"
   },
   "source": [
    "#### **2.3.2 Agente Supervisor (0.5 puntos)**\n",
    "\n",
    "Habilite un agente que tenga acceso a las tools del punto anterior y pueda responder preguntas relacionadas. Almacene este agente en una variable llamada supervisor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain.agents import Tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"RAGTool\",\n",
    "        func=rag_tool_function,\n",
    "        description=\"Utiliza la solución RAG para responder preguntas basadas en documentos PDF.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"AgentTool\",\n",
    "        func=agent_tool_function,\n",
    "        description=\"Un agente que utiliza Wikipedia y Tavily para responder preguntas.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "supervisor = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea3zWlvyvY7K"
   },
   "source": [
    "#### **2.3.3 Verificación de respuestas (0.25 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente repitiendo las preguntas realizadas en las secciones 2.1.4 y 2.2.4 y comente sus resultados. ¿Cómo varían las respuestas bajo este enfoque?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Necesito acceder al contenido del documento sobre Expected Goals (xG) para determinar su objetivo principal.  Como no tengo acceso directo a archivos PDF, usaré AgentTool para buscar información sobre el objetivo principal de los modelos xG.\n",
      "\n",
      "Action: AgentTool\n",
      "Action Input: \"¿Cuál es el objetivo principal de los modelos de Expected Goals (xG) en el fútbol?\"\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Para responder a la pregunta sobre el objetivo principal de los modelos xG en el fútbol, necesito buscar información sobre estos modelos.  Wikipedia sería una buena fuente para obtener una definición general y su propósito.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Expected Goals (xG) in football\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Expected goals\n",
      "Summary: In association football, expected goals (xG) is a performance metric u\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: ¿Cuál es el objetivo principal de los modelos de Expected Goals (xG) en el fútbol?\"\n",
      "Thought: Para responder a la pregunta sobre el objetivo principal de los modelos xG en el fútbol, necesito buscar información sobre estos modelos.  Wikipedia sería una buena fuente para obtener una definición general y su propósito.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Expected Goals (xG) in football\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Expected goals\n",
      "Summary: In association football, expected goals (xG) is a performance metric u\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: ¿Cuál es el objetivo principal de los modelos de Expected Goals (xG) en el fútbol?\"\n",
      "Thought: Para responder a la pregunta sobre el objetivo principal de los modelos xG en el fútbol, necesito buscar información sobre estos modelos. Wikipedia sería una buena fuente para obtener una definición general y su propósito.\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Expected Goals (xG) in football\"\u001b[0m\u001b[33;1m\u001b[1;3mPage: Expected goals\n",
      "Summary: In association football, expected goals (xG) is a performance metric u\u001b[0m\u001b[32;1m\u001b[1;3mThought: The previous attempts only provided a partial summary. I need to retrieve the full Wikipedia article to understand the main objective of xG models in football.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Expected Goals (xG) in football\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Expected goals\n",
      "Summary: In association football, expected goals (xG) is a performance metric u\u001b[0m\u001b[32;1m\u001b[1;3mThought: The previous attempts only provided a partial summary. I need to retrieve the full Wikipedia article to understand the main objective of xG models in football.  I will try a different approach, focusing on the *objective* of xG models.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Objective of Expected Goals (xG) in football\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Glossary of association football terms\n",
      "Summary: Association football (more commonly known as f\u001b[0m\u001b[32;1m\u001b[1;3mThought: The Wikipedia queries haven't provided a direct answer to the main objective of xG models.  I'll try a more specific query focusing on the purpose.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Purpose of Expected Goals (xG) in football\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Glossary of Australian rules football\n",
      "Summary: This list is an alphabetical glossary of Austra\u001b[0m\u001b[32;1m\u001b[1;3mThought: The Wikipedia queries are not yielding a satisfactory answer.  I will try a different approach using a broader search engine like TavilySearch, hoping to find articles or resources that explicitly state the main objective of xG models.\n",
      "\n",
      "Action: TavilySearch\n",
      "Action Input: \"main objective expected goals xG football\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://footballxg.com/what_are_expected_goals/', 'content': 'Expected goals (xG) is a metric that calculates how many goals a team should have scored based on the quality of the chances created. Learn how xG is calculated, how it can predict match outcomes, and its limitations and advantages.'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: The TavilySearch result provides a concise explanation of xG.  I can now formulate the final answer.\n",
      "\n",
      "Final Answer: El objetivo principal de los modelos de Expected Goals (xG) en el fútbol es calcular cuántos goles debería haber marcado un equipo basándose en la calidad de las ocasiones de gol creadas.  Esto permite evaluar el rendimiento ofensivo de un equipo más allá del simple número de goles anotados, considerando la calidad de las oportunidades generadas.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mEl objetivo principal de los modelos de Expected Goals (xG) en el fútbol es calcular cuántos goles debería haber marcado un equipo basándose en la calidad de las ocasiones de gol creadas.  Esto permite evaluar el rendimiento ofensivo de un equipo más allá del simple número de goles anotados, considerando la calidad de las oportunidades generadas.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: La información obtenida de AgentTool proporciona una respuesta completa a la pregunta. No necesito más acciones.\n",
      "\n",
      "Thought: I now know the final answer\n",
      "Final Answer: El objetivo principal de los modelos de Expected Goals (xG) es calcular cuántos goles debería haber marcado un equipo basándose en la calidad de las ocasiones de gol creadas, ofreciendo una evaluación del rendimiento ofensivo más allá del simple conteo de goles anotados.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "El objetivo principal de los modelos de Expected Goals (xG) es calcular cuántos goles debería haber marcado un equipo basándose en la calidad de las ocasiones de gol creadas, ofreciendo una evaluación del rendimiento ofensivo más allá del simple conteo de goles anotados.\n"
     ]
    }
   ],
   "source": [
    "respuesta1 = supervisor.run(\"¿Cuál es el objetivo principal del documento sobre Expected Goals (xG)?\")\n",
    "print(respuesta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Necesito acceder al documento de clustering de jugadores de fútbol para identificar los métodos utilizados.  Como no tengo acceso a ese documento específico, debo usar una herramienta que pueda acceder a información general sobre métodos de clustering usados en análisis deportivos.  AgentTool parece la mejor opción para esto.\n",
      "\n",
      "Action: AgentTool\n",
      "Action Input: \"¿Qué métodos de clustering se utilizan comúnmente en el análisis de datos de jugadores de fútbol?\"\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Para responder a la pregunta sobre qué métodos de clustering se utilizan comúnmente en el análisis de datos de jugadores de fútbol, necesito buscar información sobre técnicas de clustering aplicadas en el análisis deportivo.  Tanto TavilySearch como WikipediaQuery podrían ser útiles, pero WikipediaQuery podría ser más general y necesitaría una búsqueda más específica en TavilySearch si no encuentro suficiente información.  Intentaré primero con Wikipedia, buscando información sobre \"clustering\" y \"análisis deportivo\" o \"análisis de rendimiento deportivo\". Si no obtengo resultados satisfactorios, usaré TavilySearch con una consulta más específica.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"clustering\" AND (\"sports analytics\" OR \"sports performance analysis\")\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Network Science Based Basketball Analytics\n",
      "Summary: Network Science based basketball analytics\u001b[0m\u001b[32;1m\u001b[1;3mThought: La observación de Wikipedia no proporciona directamente los métodos de clustering usados en el análisis de datos de jugadores de fútbol. Necesito una búsqueda más específica y enfocada en los métodos de clustering.  Usaré TavilySearch para obtener información más precisa.\n",
      "\n",
      "Action: TavilySearch\n",
      "Action Input: \"clustering methods in soccer player data analysis\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://arxiv.org/pdf/2204.09793', 'content': \"Clustering of football players performance data Akhanli and Hennig data. Cluster analysis has been used for aggregating similar types of players in sev-eral applications.Ogles and Masters(2003) suggested that by using cluster analysis (Ward's method), marathon runners can be categorised into ﬁve groups in terms of\"}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: La respuesta de TavilySearch proporciona un ejemplo del método de clustering de Ward.  Aunque solo menciona un método, es un buen comienzo.  Para obtener una respuesta más completa, buscaré más información en TavilySearch usando términos más generales.\n",
      "\n",
      "Action: TavilySearch\n",
      "Action Input: \"clustering methods sports analytics\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.sciencedirect.com/science/article/pii/S2405844024093289', 'content': \"The rapid advancement of sports science and data analytics underscores the critical need for a sophisticated analytical approach. ... A highly intelligent clustering method of athletes' sports behavior characteristics, grounded in advanced algorithms is proposed to explore the correlation between the behavior characteristics of athletes and\"}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: La información obtenida de TavilySearch menciona el método de Ward y sugiere la existencia de otros métodos de clustering inteligentes en el análisis deportivo, pero no los especifica.  Para obtener una lista más completa de métodos comúnmente usados, necesitaré buscar en otras fuentes o refinar mis búsquedas en TavilySearch.  Sin embargo, con la información actual, puedo dar una respuesta parcial.\n",
      "\n",
      "Final Answer:  Basado en la información disponible, el método de clustering de Ward se ha utilizado en el análisis de datos de jugadores de fútbol.  Otras técnicas de clustering \"inteligentes\" también se emplean en el análisis deportivo, aunque no se especifican en las fuentes consultadas.  Se requiere una investigación más exhaustiva para obtener una lista completa de los métodos comúnmente utilizados.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mBasado en la información disponible, el método de clustering de Ward se ha utilizado en el análisis de datos de jugadores de fútbol.  Otras técnicas de clustering \"inteligentes\" también se emplean en el análisis deportivo, aunque no se especifican en las fuentes consultadas.  Se requiere una investigación más exhaustiva para obtener una lista completa de los métodos comúnmente utilizados.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: La respuesta de AgentTool es parcial.  Necesitaría más información para dar una respuesta completa.  Sin embargo, la respuesta indica que al menos un método (Ward) es usado.  Puedo intentar refinar la pregunta a AgentTool para obtener más detalles.\n",
      "\n",
      "Action: AgentTool\n",
      "Action Input: \"¿Qué algoritmos de clustering, además del método de Ward, se utilizan en el análisis de rendimiento de jugadores de fútbol?  Menciona ejemplos concretos si es posible.\"\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Para responder a la pregunta sobre algoritmos de clustering usados en el análisis de rendimiento de jugadores de fútbol, además del método de Ward, necesito buscar información en una base de datos o enciclopedia que trate temas deportivos y análisis de datos.  Wikipedia podría tener información general sobre algoritmos de clustering, pero es menos probable que tenga ejemplos específicos de su aplicación en el análisis de rendimiento deportivo.  Una búsqueda en Tavily (asumiendo que es una base de datos o motor de búsqueda similar a Google Scholar) podría ser más fructífera.  Intentaré primero con Tavily, y si no obtengo resultados satisfactorios, recurriría a Wikipedia.\n",
      "\n",
      "Action: TavilySearch\n",
      "Action Input: \"algoritmos clustering análisis rendimiento jugadores fútbol métodos Ward\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://uvadoc.uva.es/handle/10324/57954', 'content': 'Uso de técnicas de clustering para encontrar perfiles de jugadores en una competición de fútbol profesional ... Resumen. El Big Data se ha ido haciendo un hueco en el mundo del fútbol, convirtiendose en un fijo en el personal de la mayoria de equipos, siendo fundamental en el análisis del rendimiento de los jugadores tanto en los partidos'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: La observación de TavilySearch proporciona un enlace a un documento que parece relevante.  Sin embargo, no especifica qué algoritmos de clustering se utilizan.  Intentaré acceder al documento (si es posible) para obtener más información. Si no puedo acceder al contenido, usaré Wikipedia para obtener información general sobre algoritmos de clustering y luego intentaré inferir su aplicabilidad al análisis de rendimiento de jugadores de fútbol.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"clustering algorithms\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: K-means clustering\n",
      "Summary: k-means clustering is a method of vector quantization, originally \u001b[0m\u001b[32;1m\u001b[1;3mThought: La búsqueda en Wikipedia sobre \"clustering algorithms\" proporciona información general, pero no ejemplos específicos en el contexto del análisis de rendimiento de jugadores de fútbol.  Necesitaré buscar más información para responder completamente a la pregunta.  Intentaré una búsqueda más específica en Tavily.\n",
      "\n",
      "Action: TavilySearch\n",
      "Action Input: \"clustering algorithms football player performance analysis k-means hierarchical clustering DBSCAN\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://github.com/SaySohail/football-player-similarity-kmeans-gmm', 'content': 'Two clustering algorithms, K-means and Gaussian Mixture Model (GMM), were used to cluster the football players based on their performance data. 4.1 K-means K-means clustering is an algorithm that involves specifying the number of clusters (K) and iteratively assigning each data point to one of the clusters based on the similarity of their features.'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: La observación de TavilySearch ahora proporciona un enlace a un repositorio de GitHub que menciona explícitamente el uso de K-means y Gaussian Mixture Model (GMM) para el clustering de jugadores de fútbol basados en su rendimiento.  Esto responde a la pregunta.\n",
      "\n",
      "Final Answer: Además del método de Ward, algoritmos como K-means y Gaussian Mixture Model (GMM) se utilizan en el análisis de rendimiento de jugadores de fútbol.  Un ejemplo concreto es el uso de K-means y GMM para agrupar jugadores según sus datos de rendimiento, como se muestra en un repositorio de GitHub (enlace proporcionado en la observación anterior).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mAdemás del método de Ward, algoritmos como K-means y Gaussian Mixture Model (GMM) se utilizan en el análisis de rendimiento de jugadores de fútbol.  Un ejemplo concreto es el uso de K-means y GMM para agrupar jugadores según sus datos de rendimiento, como se muestra en un repositorio de GitHub (enlace proporcionado en la observación anterior).\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: La respuesta de AgentTool ahora proporciona una lista más completa de métodos de clustering.  Puedo usar esta información para formular mi respuesta final.\n",
      "\n",
      "Thought: I now know the final answer\n",
      "Final Answer:  Basado en la información disponible, los métodos de clustering comúnmente utilizados en el análisis de datos de jugadores de fútbol incluyen el método de Ward, K-means y Gaussian Mixture Model (GMM).  Es posible que se utilicen otros métodos, pero estos tres son ejemplos concretos.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Basado en la información disponible, los métodos de clustering comúnmente utilizados en el análisis de datos de jugadores de fútbol incluyen el método de Ward, K-means y Gaussian Mixture Model (GMM).  Es posible que se utilicen otros métodos, pero estos tres son ejemplos concretos.\n"
     ]
    }
   ],
   "source": [
    "respuesta2 = supervisor.run(\"¿Qué métodos de clustering se utilizan en el documento de clustering de jugadores de fútbol?\")\n",
    "print(respuesta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: No tengo conocimiento previo sobre el término \"conceptor\" en el análisis de juegos de fútbol.  Intentaré usar AgentTool para buscar información en Wikipedia y Tavily.\n",
      "\n",
      "Action: AgentTool\n",
      "Action Input: \"¿Qué es un 'conceptor' en el contexto del análisis de juegos de fútbol?\"\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Necesito encontrar información sobre el término \"conceptor\" en el contexto del análisis de juegos de fútbol.  Como es un término específico del deporte, es probable que Wikipedia no tenga una entrada dedicada.  Intentaré primero con una búsqueda en Tavily, que podría tener información de blogs, foros o sitios web especializados en análisis futbolístico.\n",
      "\n",
      "Action: TavilySearch\n",
      "Action Input: \"conceptor análisis juego fútbol\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://futboldelibro.com/wp-content/uploads/2019/10/MuestraModeloDeJuego.pdf', 'content': 'MODELO DE JUEGO: ESTRUCTURA, METODOLOGÍA Y APLICACIÓN PRÁCTICA. ... cursos y sabiendo la evolución y revolución que ha tenido el fútbol a nivel de conceptos y nuevas formas de entrenamientos y entendimientos; es de admirar la ... del análisis del rival, que puede modificar aspectos de nuestro juego en momentos'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: La observación de TavilySearch proporciona un PDF que parece tratar sobre modelos de juego en fútbol, pero no define explícitamente \"conceptor\".  Necesitaré más información. Intentaré una búsqueda más específica en Tavily, incluyendo sinónimos o términos relacionados.\n",
      "\n",
      "Action: TavilySearch\n",
      "Action Input: \"jugador creador juego fútbol análisis táctico\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://quicklineup.com/es', 'content': 'Usa nuestro creador de plantilla de fútbol para formar tu equipo de 11 jugadores. Crea, personaliza y comparte tu alineación de fútbol ideal con Quick Lineup. ... El fútbol no es solo un juego de once jugadores, sino también una danza de tácticas, estrategias e imaginación. ... Análisis Táctico ¡Tienes muchas opciones para probar'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: La segunda búsqueda en Tavily tampoco proporciona una definición directa de \"conceptor\".  Las respuestas parecen apuntar a recursos relacionados con el análisis táctico y la creación de plantillas, pero no definen el término.  Intentaré una búsqueda en Wikipedia sobre conceptos relacionados con el análisis táctico del fútbol, esperando encontrar alguna referencia indirecta.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Análisis táctico fútbol\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Liga Nacional de Fútbol Profesional\n",
      "Summary: The Liga Nacional de Fútbol Profesional (transl. \u001b[0m\u001b[32;1m\u001b[1;3mThought: La búsqueda en Wikipedia no arrojó una definición directa de \"conceptor\" en el contexto del análisis de fútbol.  La información obtenida de Tavily tampoco fue útil.  Es probable que \"conceptor\" sea un término menos común o utilizado en un contexto específico, quizás dentro de una escuela de pensamiento particular sobre el análisis táctico del fútbol.  Sin más información, no puedo definir el término.\n",
      "\n",
      "Final Answer: No se encontró una definición de \"conceptor\" en el contexto del análisis de juegos de fútbol utilizando las herramientas disponibles.  Es posible que el término sea poco común o específico de un ámbito particular del análisis táctico.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mNo se encontró una definición de \"conceptor\" en el contexto del análisis de juegos de fútbol utilizando las herramientas disponibles.  Es posible que el término sea poco común o específico de un ámbito particular del análisis táctico.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: La búsqueda en AgentTool no tuvo éxito.  Podría ser que el término sea muy específico o un neologismo. Intentaré reformular la pregunta para ver si obtengo mejores resultados.\n",
      "\n",
      "Action: AgentTool\n",
      "Action Input: \"Definir 'conceptor' en el contexto del fútbol.  Si no existe una definición específica, describir roles de jugadores de fútbol que podrían ser considerados 'conceptores' en un análisis táctico.\"\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: No existe una definición estándar de \"conceptor\" en el contexto del fútbol.  Debería buscar en Wikipedia información sobre roles de jugadores de fútbol y luego intentar identificar cuáles podrían considerarse \"conceptores\" basándome en un análisis táctico.  Primero buscaré en Wikipedia para ver si hay alguna mención, aunque sea indirecta.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"roles de jugadores de fútbol\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Liga Nacional de Fútbol Profesional\n",
      "Summary: The Liga Nacional de Fútbol Profesional (transl. \u001b[0m\u001b[32;1m\u001b[1;3mThought:The Wikipedia query about \"roles de jugadores de fútbol\" returned a page about a specific football league, not a general overview of player roles.  I need a more general search to find information about different player positions and their tactical functions.  I will try a different Wikipedia query.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"posiciones de fútbol\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2011 Liga de Fútbol Profesional Boliviano season\n",
      "Summary: The 2011 Liga de Fútbol Profesional \u001b[0m\u001b[32;1m\u001b[1;3mThought:The Wikipedia query about \"posiciones de fútbol\" also returned a page about a specific football league, not a general overview of player positions.  I need to try a different approach.  I will try searching for specific player roles and then infer which might fit the \"conceptor\" description.  Since \"conceptor\" implies a player who creates plays, I'll look for roles associated with playmaking.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"playmaker fútbol\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Envigado F.C.\n",
      "Summary: Envigado Fútbol Club (Spanish pronunciation: [embiˈɣaðo ˈfuðβol ˈkluβ])\u001b[0m\u001b[32;1m\u001b[1;3mThought:The Wikipedia query about \"playmaker fútbol\" returned a page about a specific football club, not information about the playmaker role.  I will try a different search term.  I will use a more general search on Wikipedia to find information about different attacking midfield roles.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"mediocampista ofensivo fútbol\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Mariano Miño\n",
      "Summary: Alan Mariano Miño (born March 28, 1994) is an Argentinian footballer who\u001b[0m\u001b[32;1m\u001b[1;3mThought:The Wikipedia queries haven't yielded a satisfactory definition of \"conceptor\" in football or a list of player roles that directly use that term.  However, the searches have highlighted the difficulty of finding a precise definition.  I will now attempt to infer potential roles based on the implied meaning of \"conceptor,\" which suggests a player who conceives or creates attacking plays.\n",
      "\n",
      "Thought: I will now describe roles that could be considered \"conceptores\" based on their tactical functions.\n",
      "\n",
      "Thought: I now know the final answer.\n",
      "\n",
      "Final Answer: No existe una definición formal de \"conceptor\" en el fútbol. Sin embargo, basándonos en la connotación de crear o concebir jugadas, varios roles podrían considerarse \"conceptores\" en un análisis táctico.  Estos incluyen, pero no se limitan a:\n",
      "\n",
      "* **Mediocampista ofensivo/creativo:** Este jugador es el cerebro del equipo, dictando el ritmo del juego y creando oportunidades de gol a través de pases precisos, visión de juego y habilidad técnica.  Su función principal es la concepción de las jugadas ofensivas.\n",
      "\n",
      "* **Mediapunta:**  Similar al mediocampista ofensivo, pero a menudo con una posición más adelantada,  la media punta se encarga de conectar el mediocampo con la delantera, creando y finalizando jugadas.\n",
      "\n",
      "* **Delantero falso (False 9):**  Aunque nominalmente un delantero, este jugador se mueve con libertad por todo el frente de ataque, creando espacios y oportunidades para sus compañeros a través de su movilidad y visión de juego.  Su función es menos la finalización y más la creación de jugadas.\n",
      "\n",
      "* **Lateral ofensivo:** En sistemas modernos, los laterales ofensivos participan activamente en la creación de juego, subiendo por la banda y proporcionando centros y pases clave.  Su contribución a la concepción de jugadas es significativa.\n",
      "\n",
      "\n",
      "La designación de un jugador como \"conceptor\" dependerá del sistema táctico del equipo y del rol específico que ese jugador desempeña dentro de él.  No es un término formal, sino más bien una interpretación analítica.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mNo existe una definición formal de \"conceptor\" en el fútbol. Sin embargo, basándonos en la connotación de crear o concebir jugadas, varios roles podrían considerarse \"conceptores\" en un análisis táctico.  Estos incluyen, pero no se limitan a:\n",
      "\n",
      "* **Mediocampista ofensivo/creativo:** Este jugador es el cerebro del equipo, dictando el ritmo del juego y creando oportunidades de gol a través de pases precisos, visión de juego y habilidad técnica.  Su función principal es la concepción de las jugadas ofensivas.\n",
      "\n",
      "* **Mediapunta:**  Similar al mediocampista ofensivo, pero a menudo con una posición más adelantada,  la media punta se encarga de conectar el mediocampo con la delantera, creando y finalizando jugadas.\n",
      "\n",
      "* **Delantero falso (False 9):**  Aunque nominalmente un delantero, este jugador se mueve con libertad por todo el frente de ataque, creando espacios y oportunidades para sus compañeros a través de su movilidad y visión de juego.  Su función es menos la finalización y más la creación de jugadas.\n",
      "\n",
      "* **Lateral ofensivo:** En sistemas modernos, los laterales ofensivos participan activamente en la creación de juego, subiendo por la banda y proporcionando centros y pases clave.  Su contribución a la concepción de jugadas es significativa.\n",
      "\n",
      "\n",
      "La designación de un jugador como \"conceptor\" dependerá del sistema táctico del equipo y del rol específico que ese jugador desempeña dentro de él.  No es un término formal, sino más bien una interpretación analítica.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer.  The AgentTool provided a useful response, even though \"conceptor\" isn't a formally defined term in football analysis.\n",
      "\n",
      "Final Answer: No existe una definición formal de \"conceptor\" en el análisis de fútbol. Sin embargo, el término se refiere a jugadores que crean y conciben jugadas ofensivas.  Esto podría incluir mediocampistas ofensivos/creativos, mediapuntas, delanteros falsos y laterales ofensivos, dependiendo del sistema táctico del equipo y del rol específico del jugador.  La designación de \"conceptor\" es una interpretación analítica, no un término técnico establecido.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "No existe una definición formal de \"conceptor\" en el análisis de fútbol. Sin embargo, el término se refiere a jugadores que crean y conciben jugadas ofensivas.  Esto podría incluir mediocampistas ofensivos/creativos, mediapuntas, delanteros falsos y laterales ofensivos, dependiendo del sistema táctico del equipo y del rol específico del jugador.  La designación de \"conceptor\" es una interpretación analítica, no un término técnico establecido.\n"
     ]
    }
   ],
   "source": [
    "respuesta3 = supervisor.run(\"¿Qué es un 'conceptor' en el contexto del análisis de juegos de fútbol?\")\n",
    "print(respuesta3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Necesito encontrar información sobre el ganador de la Copa Mundial de la FIFA 2022.  Puedo usar AgentTool ya que tiene acceso a Wikipedia, que seguramente tendrá esta información.\n",
      "\n",
      "Action: AgentTool\n",
      "Action Input: ¿Qué equipo ganó la Copa Mundial de la FIFA 2022?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Necesito encontrar información sobre la Copa Mundial de la FIFA 2022 para determinar qué equipo la ganó.  Wikipedia es una buena fuente para este tipo de información.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Copa Mundial de la FIFA 2022\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Copa América de Futsal\n",
      "Summary: The CONMEBOL Copa América de Futsal is the main national futsa\u001b[0m\u001b[32;1m\u001b[1;3mThought: The observation is incorrect; it's about Futsal, not the FIFA World Cup. I need to try again with a more specific query.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"2022 FIFA World Cup Winner\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mThought: The observation mentions the 2022 FIFA World Cup. I need to extract the winner from the Wikipedia page summary or content.  I'll try a more direct query.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"2022 FIFA World Cup winner\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mThought: The summary still doesn't explicitly state the winner. I need to access the full Wikipedia page content.  I'll try a different approach.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: {\"query\": \"2022 FIFA World Cup winner\", \"extract\": True}\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\u001b[32;1m\u001b[1;3mThought: The previous attempts failed to extract the winner directly.  Let's try a different approach using a more specific query to Wikipedia.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Winner of the 2022 FIFA World Cup\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: 2022 FIFA World Cup\n",
      "Summary: The 2022 FIFA World Cup was the 22nd FIFA World Cup, the quadrenn\u001b[0m\u001b[32;1m\u001b[1;3mThought: The summary still doesn't give the winner. I need to look for the winner within the full Wikipedia article.  Since I'm having trouble extracting the information directly, I'll try a different strategy.  I'll search for the information using a search engine via TavilySearch.\n",
      "\n",
      "Action: TavilySearch\n",
      "Action Input: \"2022 FIFA World Cup winner\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.espn.com/soccer/match/_/gameId/633850/france-argentina', 'content': 'Game summary of the Argentina vs. France Fifa World Cup game, final score 3-3, from December 18, 2022 on ESPN.'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: The TavilySearch result mentions a game between Argentina and France.  While it doesn't explicitly state the winner, the context strongly suggests a close match.  I need to find a source that clearly states the winner.  Let's try Wikipedia again, focusing on the infobox.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: {\"query\": \"2022 FIFA World Cup\", \"extract_infobox\": True}\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\u001b[32;1m\u001b[1;3mThought: The previous attempts haven't yielded a clear winner.  Let's try a different search strategy on TavilySearch, focusing on a more concise query.\n",
      "\n",
      "Action: TavilySearch\n",
      "Action Input: \"Who won the 2022 World Cup?\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.sportingnews.com/us/soccer/news/who-won-2022-fifa-world-cup-final-score-result-highlights-qatar/aar5gfviuuapvkmcq3tf7dep', 'content': \"From the moment Lionel Messi put the Copa America holders ahead in the 23rd minute of the 2022 FIFA World Cup final against France, Lionel Scaloni's side looked determined to deny the reigning champions as they survived a late comeback and penalties to storm to a stylish triumph in the first ever winter tournament in Qatar.\\n Here's how the penalty shootout played out:\\nFIFA 2022 World Cup final highlights\\nHere's how Messi's first goal looked\\nUS viewers\\nMESSI PUTS ARGENTINA OUT IN FRONT 🎯🇦🇷 pic.twitter.com/kZkUvRUXC9\\nCanada viewers\\nLEO MESSI PUTS ARGENTINA UP 1-0! 🇦🇷#FIFAWorldCup pic.twitter.com/xZcTiKyfWv\\nUK viewers\\nMbappe makes it 3-3 from the spot\\nUS viewers\\nMBAPPE TIES IT AGAIN3-3 IN THE 117TH MINUTE 😱 pic.twitter.com/EelVTJMRiI\\nCanada viewers\\nMBAPPE EVENS IT UP ONCE AGAIN! #FIFAWorldCup pic.twitter.com/0NgO1CW0xI\\nUK viewers\\nMessi - Mac Allister - Di Maria 🇦🇷France are caught out on a swift counter attack as Argentina take control of the game with their second goal... 🙌#ITVFootball | #FIFAWorldCup pic.twitter.com/QCsZSYCuPL\\nThis is how Mbappe gave France hope\\nUS viewers\\nMBAPPE TAKES ONE BACK FOR FRANCE 🇫🇷Game. 🇫🇷#ITVFootball | #FIFAWorldCup pic.twitter.com/idYQRqrZtv\\nMessi restores Argentina's lead in extra time\\nUS viewers\\nIT HAD TO BE MESSI 🐐🇦🇷 pic.twitter.com/KF5wggitVF\\nCanada viewers\\nLEO MESSI HAS PUT ARGENTINA ON TOP!\"}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: The TavilySearch result clearly states that Argentina won the 2022 FIFA World Cup.\n",
      "\n",
      "Final Answer: Argentina\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mArgentina\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer.\n",
      "Final Answer: Argentina\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Argentina\n"
     ]
    }
   ],
   "source": [
    "response = supervisor.run(\"qué equipo ganó el mundial de fútbol 2022?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para las preguntas que iban dirigidas al RAG, este enfoque entregó respuestas más generales ya que buscó información en la web, por lo que no es tan específico aplicado a los documentos como se quisiera. Para la respuesta que fue enfocada al trabajo del agente, la respuesta y el trabajo por detrás fue casi lo mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qb8bdAmYvgwn"
   },
   "source": [
    "#### **2.3.4 Análisis (0.25 puntos)**\n",
    "\n",
    "¿Qué diferencias tiene este enfoque con la solución *Router* vista en clases? Nombre al menos una ventaja y desventaja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAUlJxqoLK5r"
   },
   "source": [
    "Este enfoque como es supervisor, trabaja con los distintos llms de modo de obtener una respuesta, a diferencia del router que redigire la pregunta a un llm específico y se obtiene una sola respuesta. La ventaja de este enfoque es que obtiene respuestas más robustas y con más trabajo por detrás, es decir, se espera que tengan más sentido por más que sean más resumidas en unos casos, a diferencia de la solución router que solo entrega la respuesta del llm solicitado. \n",
    "\n",
    "La desventaja de este enfoque son los tiempos de ejecución y el costo computacional que hay, ya que para algunas preguntas simples se puede tardar minutos por complejizar el trabajo detrás utilizando varios llms, a diferencia de la solución router, que al tratar con un llm no es necesario hacer más trabajo por detrás, por lo que la respuesta es más rápida y más eficiente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JWVSuWiZ8Mj"
   },
   "source": [
    "### **2.4 Memoria (Bonus +0.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/Gs95aiElrscAAAAd/memory-unlocked-ratatouille-critic.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una de las principales falencias de las soluciones que hemos visto hasta ahora es que nuestro chat no responde las interacciones anteriores, por ejemplo:\n",
    "\n",
    "- Pregunta 1: \"Hola! mi nombre es Sebastián\"\n",
    "  - Respuesta esperada: \"Hola Sebastián! ...\"\n",
    "- Pregunta 2: \"Cual es mi nombre?\"\n",
    "  - Respuesta actual: \"Lo siento pero no conozco tu nombre :(\"\n",
    "  - **Respuesta esperada: \"Tu nombre es Sebastián\"**\n",
    "\n",
    "Para solucionar esto, se les solicita agregar un componente de **memoria** a la solución entregada en el punto 2.3.\n",
    "\n",
    "**Nota: El Bonus es válido <u>sólo para la sección 2 de Large Language Models.</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6Y7tIPJLPfB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFc3jBT5g0kT"
   },
   "source": [
    "### **2.5 Despliegue (0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/IytHqOp52EsAAAAd/you-get-a-deploy-deploy.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una vez tengan los puntos anteriores finalizados, toca la etapa de dar a conocer lo que hicimos! Para eso, vamos a desplegar nuestro modelo a través de `gradio`, una librería especializada en el levantamiento rápido de demos basadas en ML.\n",
    "\n",
    "Primero instalamos la librería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "T8TsvnCPbkIA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJBztEUovKsF"
   },
   "source": [
    "Luego sólo deben ejecutar el siguiente código e interactuar con la interfaz a través del notebook o del link generado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3KedQSvg1-n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* Running on public URL: https://33b3d3e55626aba88d.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://33b3d3e55626aba88d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to define \"expected goals\".  Since it's a statistical concept likely related to sports, I'll try AgentTool first, as it has access to Wikipedia which is a good source for definitions.\n",
      "\n",
      "Action: AgentTool\n",
      "Action Input: Define \"expected goals\" in soccer (football).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To define \"expected goals\" in soccer, I need to find a reliable source of information, such as Wikipedia or a reputable sports statistics website.  Wikipedia is a good starting point for a general definition.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Expected goals soccer\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Expected goals\n",
      "Summary: In association football, expected goals (xG) is a performance metric u\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Define \"expected goals\" in soccer (football).\n",
      "Thought:To define \"expected goals\" in soccer, I need to find a reliable source of information, such as Wikipedia or a reputable sports statistics website.  Wikipedia is a good starting point for a general definition.\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Expected goals soccer\"\u001b[0m\u001b[33;1m\u001b[1;3mPage: Expected goals\n",
      "Summary: In association football, expected goals (xG) is a performance metric u\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Define \"expected goals\" in soccer (football).\n",
      "Thought:To define \"expected goals\" in soccer, I need to find a reliable source of information, such as Wikipedia or a reputable sports statistics website.  Wikipedia is a good starting point for a general definition.\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Expected goals soccer\"\u001b[0m\u001b[33;1m\u001b[1;3mPage: Expected goals\n",
      "Summary: In association football, expected goals (xG) is a performance metric u\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Define \"expected goals\" in soccer (football).\n",
      "\n",
      "Thought:To define \"expected goals\" in soccer, I need to find a reliable source of information, such as Wikipedia.\n",
      "\n",
      "Action: WikipediaQuery\n",
      "Action Input: \"Expected goals soccer\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Expected goals\n",
      "Summary: In association football, expected goals (xG) is a performance metric u\u001b[0m"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "def agent_response(message, history):\n",
    "  '''\n",
    "  Función para gradio, recibe mensaje e historial, devuelte la respuesta del chatbot.\n",
    "  '''\n",
    "  # get chatbot response\n",
    "  response = supervisor.run(message)\n",
    "\n",
    "  # assert\n",
    "  assert type(response) == str, \"output de route_question debe ser string\"\n",
    "\n",
    "  # \"streaming\" response\n",
    "  for i in range(len(response)):\n",
    "    time.sleep(0.015)\n",
    "    yield response[: i+1]\n",
    "\n",
    "gr.ChatInterface(\n",
    "    agent_response,\n",
    "    type=\"messages\",\n",
    "    title=\"Chatbot Clustering Football MDS7202\", # Pueden cambiar esto si lo desean\n",
    "    description=\"Hola! Soy un chatbot con información de ML en el fútbol\", # también la descripción\n",
    "    theme=\"soft\",\n",
    "    ).launch(\n",
    "        share=True, # pueden compartir el link a sus amig@s para que interactuen con su chat!\n",
    "        debug = False,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
